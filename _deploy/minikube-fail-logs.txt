
==> Audit <==
|---------|------------------------------------------------------------------------------------------------------|----------|--------|---------|---------------------|---------------------|
| Command |                                                 Args                                                 | Profile  |  User  | Version |     Start Time      |      End Time       |
|---------|------------------------------------------------------------------------------------------------------|----------|--------|---------|---------------------|---------------------|
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 14:22 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto                                                                            |          |        |         |                     |                     |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 14:22 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto                                                                            |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 14:25 MSK | 18 Jan 24 14:25 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 14:25 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto                                                                            |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 14:33 MSK | 18 Jan 24 14:33 MSK |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 15:48 MSK | 18 Jan 24 15:48 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 17:07 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:10 MSK |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:25 MSK | 18 Jan 24 17:25 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 17:29 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:31 MSK |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:32 MSK | 18 Jan 24 17:32 MSK |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:34 MSK |                     |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 17:34 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:36 MSK | 18 Jan 24 17:37 MSK |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:37 MSK | 18 Jan 24 17:40 MSK |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:40 MSK | 18 Jan 24 17:40 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 17:40 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:41 MSK | 18 Jan 24 17:41 MSK |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:41 MSK | 18 Jan 24 17:41 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 17:41 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 17:47 MSK | 18 Jan 24 17:47 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 20:06 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:09 MSK |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:16 MSK | 18 Jan 24 20:16 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 20:16 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:17 MSK |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:19 MSK |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:23 MSK | 18 Jan 24 20:23 MSK |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:24 MSK | 18 Jan 24 20:24 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 20:24 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:25 MSK |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:29 MSK |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:33 MSK | 18 Jan 24 20:33 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 20:33 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:34 MSK |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:35 MSK |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:41 MSK | 18 Jan 24 20:41 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 18 Jan 24 20:41 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| ssh     |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:42 MSK | 18 Jan 24 20:42 MSK |
| start   | --iso-url=file:///tmp/minikube.iso                                                                   | minikube | ernado | v1.32.0 | 18 Jan 24 20:49 MSK |                     |
|         | --cni=cilium                                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto                                                                            |          |        |         |                     |                     |
|         | --cache-images=false                                                                                 |          |        |         |                     |                     |
| start   | --iso-url=file:///tmp/minikube.iso                                                                   | minikube | ernado | v1.32.0 | 18 Jan 24 20:49 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto                                                                            |          |        |         |                     |                     |
|         | --cache-images=false                                                                                 |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 18 Jan 24 20:49 MSK | 18 Jan 24 20:49 MSK |
| start   | --iso-url=file:///tmp/minikube.iso                                                                   | minikube | ernado | v1.32.0 | 18 Jan 24 20:49 MSK |                     |
|         | --driver=vmware --cni=cilium                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto                                                                            |          |        |         |                     |                     |
|         | --cache-images=false                                                                                 |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 19 Jan 24 07:36 MSK | 19 Jan 24 07:36 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 28 Jan 24 14:42 MSK |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 28 Jan 24 14:57 MSK |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 28 Jan 24 14:58 MSK |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 28 Jan 24 15:05 MSK | 28 Jan 24 15:05 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 28 Jan 24 15:12 MSK | 28 Jan 24 15:13 MSK |
|         | --cache-images=false                                                                                 |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 28 Jan 24 15:20 MSK | 28 Jan 24 15:20 MSK |
| start   | --iso-url=file:///src/minikube/out/minikube-amd64.iso                                                | minikube | ernado | v1.32.0 | 28 Jan 24 15:20 MSK |                     |
|         | --container-runtime=porto --cache-images=false                                                       |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 28 Jan 24 15:22 MSK | 28 Jan 24 15:22 MSK |
| start   | --iso-url=https://github.com/go-faster/minikube/releases/download/v1.32.1-alpha.1/minikube-amd64.iso | minikube | ernado | v1.32.0 | 28 Jan 24 15:34 MSK |                     |
|         | --cni=cilium --container-runtime=porto --cache-images=false                                          |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 28 Jan 24 15:35 MSK | 28 Jan 24 15:35 MSK |
| start   | --iso-url=file///tmp/minikube.iso                                                                    | minikube | ernado | v1.32.0 | 28 Jan 24 15:36 MSK |                     |
|         | --cni=cilium                                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto                                                                            |          |        |         |                     |                     |
|         | --cache-images=false                                                                                 |          |        |         |                     |                     |
| start   | --iso-url=file///tmp/minikube.iso                                                                    | minikube | ernado | v1.32.0 | 28 Jan 24 15:36 MSK |                     |
|         | --cni=cilium                                                                                         |          |        |         |                     |                     |
|         | --container-runtime=porto                                                                            |          |        |         |                     |                     |
|         | --cache-images=false                                                                                 |          |        |         |                     |                     |
| start   | --iso-url=file:///src/faster/ytst/deploy/minikube-amd64.iso                                          | minikube | ernado | v1.32.0 | 28 Jan 24 15:38 MSK |                     |
|         | --cni=cilium --container-runtime=porto --cache-images=false                                          |          |        |         |                     |                     |
| delete  |                                                                                                      | minikube | ernado | v1.32.0 | 28 Jan 24 15:41 MSK | 28 Jan 24 15:41 MSK |
| start   | --iso-url=file:///src/faster/ytst/deploy/minikube-amd64.iso                                          | minikube | ernado | v1.32.0 | 28 Jan 24 15:41 MSK |                     |
|         | --cni=cilium --container-runtime=porto --cache-images=false                                          |          |        |         |                     |                     |
|---------|------------------------------------------------------------------------------------------------------|----------|--------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/01/28 15:41:47
Running on machine: work
Binary: Built with gc go1.21.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0128 15:41:47.100739  492188 out.go:296] Setting OutFile to fd 1 ...
I0128 15:41:47.101066  492188 out.go:348] isatty.IsTerminal(1) = true
I0128 15:41:47.101069  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:41:47.101071  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:41:47.101179  492188 root.go:338] Updating PATH: /home/ernado/.minikube/bin
I0128 15:41:47.101479  492188 out.go:303] Setting JSON to false
I0128 15:41:47.103577  492188 start.go:130] hostinfo: {"hostname":"work","uptime":2963,"bootTime":1706442744,"procs":744,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-15-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"f8eabc3f-2fba-40ac-887c-a8caeb57f100"}
I0128 15:41:47.103611  492188 start.go:140] virtualization: kvm host
I0128 15:41:47.106391  492188 out.go:177] üòÑ  minikube v1.32.0 on Ubuntu 22.04
I0128 15:41:47.107410  492188 driver.go:392] Setting default libvirt URI to qemu:///system
I0128 15:41:47.107414  492188 notify.go:220] Checking for updates...
I0128 15:41:47.185137  492188 out.go:177] ‚ú®  Using the kvm2 driver based on user configuration
I0128 15:41:47.186955  492188 start.go:300] selected driver: kvm2
I0128 15:41:47.186958  492188 start.go:906] validating driver "kvm2" against <nil>
I0128 15:41:47.186963  492188 start.go:917] status for kvm2: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0128 15:41:47.187624  492188 install.go:52] acquiring lock: {Name:mk900956b073697a4aa6c80a27c6bb0742a99a53 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0128 15:41:47.187654  492188 install.go:117] Validating docker-machine-driver-kvm2, PATH=/home/ernado/.minikube/bin:/usr/local/go/bin:/go/bin:/home/ernado/yandex-cloud/bin:/home/ernado/.bun/bin:/home/ernado/.nvm/versions/node/v20.1.0/bin:/home/ernado/.cargo/bin:/home/ernado/.local/bin:/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/home/ernado/.local/share/JetBrains/Toolbox/scripts:/home/ernado/.local/share/JetBrains/Toolbox/scripts:/home/ernado/.bin:/home/ernado/.local/bin
I0128 15:41:47.196379  492188 install.go:137] /home/ernado/.minikube/bin/docker-machine-driver-kvm2 version is 1.32.0
I0128 15:41:47.196397  492188 start_flags.go:309] no existing cluster config was found, will generate one from the flags 
I0128 15:41:47.197851  492188 start_flags.go:394] Using suggested 6000MB memory alloc based on sys=127935MB, container=0MB
I0128 15:41:47.197924  492188 start_flags.go:913] Wait components to verify : map[apiserver:true system_pods:true]
I0128 15:41:47.197956  492188 cni.go:84] Creating CNI manager for "cilium"
I0128 15:41:47.197963  492188 start_flags.go:318] Found "Cilium" CNI - setting NetworkPlugin=cni
I0128 15:41:47.197969  492188 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase-builds:v0.0.42-1704751654-17830@sha256:cabd32f8d9e8d804966eb117ed5366660f6363a4d1415f0b5480de6e396be617 Memory:6000 CPUs:2 DiskSize:20000 VMDriver: Driver:kvm2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:porto CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:false EnableDefaultCNI:false CNI:cilium NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ernado:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0128 15:41:47.199015  492188 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0128 15:41:47.199916  492188 preload.go:132] Checking if preload exists for k8s version v1.28.4 and runtime porto
W0128 15:41:47.419463  492188 preload.go:115] https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.28.4/preloaded-images-k8s-v18-v1.28.4-porto-overlay2-amd64.tar.lz4 status code: 404
I0128 15:41:47.419693  492188 profile.go:148] Saving config to /home/ernado/.minikube/profiles/minikube/config.json ...
I0128 15:41:47.419709  492188 lock.go:35] WriteFile acquiring /home/ernado/.minikube/profiles/minikube/config.json: {Name:mk096e517c2212573df51cbb63a5cca6efd32347 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:41:47.419874  492188 start.go:365] acquiring machines lock for minikube: {Name:mk387589c2ecacb67be13cfd3014518e020d490d Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0128 15:41:47.419893  492188 start.go:369] acquired machines lock for "minikube" in 13.816¬µs
I0128 15:41:47.419899  492188 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:file:///src/faster/ytst/deploy/minikube-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase-builds:v0.0.42-1704751654-17830@sha256:cabd32f8d9e8d804966eb117ed5366660f6363a4d1415f0b5480de6e396be617 Memory:6000 CPUs:2 DiskSize:20000 VMDriver: Driver:kvm2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:porto CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:false EnableDefaultCNI:false CNI:cilium NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.28.4 ContainerRuntime:porto ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ernado:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name: IP: Port:8443 KubernetesVersion:v1.28.4 ContainerRuntime:porto ControlPlane:true Worker:true}
I0128 15:41:47.419935  492188 start.go:125] createHost starting for "" (driver="kvm2")
I0128 15:41:47.423419  492188 out.go:204] üî•  Creating kvm2 VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
I0128 15:41:47.423489  492188 main.go:141] libmachine: Found binary path at /home/ernado/.minikube/bin/docker-machine-driver-kvm2
I0128 15:41:47.423504  492188 main.go:141] libmachine: Launching plugin server for driver kvm2
I0128 15:41:47.431793  492188 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:35445
I0128 15:41:47.432009  492188 main.go:141] libmachine: () Calling .GetVersion
I0128 15:41:47.432251  492188 main.go:141] libmachine: Using API Version  1
I0128 15:41:47.432257  492188 main.go:141] libmachine: () Calling .SetConfigRaw
I0128 15:41:47.432379  492188 main.go:141] libmachine: () Calling .GetMachineName
I0128 15:41:47.432434  492188 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0128 15:41:47.432487  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:41:47.432529  492188 start.go:159] libmachine.API.Create for "minikube" (driver="kvm2")
I0128 15:41:47.432538  492188 client.go:168] LocalClient.Create starting
I0128 15:41:47.432548  492188 main.go:141] libmachine: Reading certificate data from /home/ernado/.minikube/certs/ca.pem
I0128 15:41:47.432564  492188 main.go:141] libmachine: Decoding PEM data...
I0128 15:41:47.432570  492188 main.go:141] libmachine: Parsing certificate...
I0128 15:41:47.432595  492188 main.go:141] libmachine: Reading certificate data from /home/ernado/.minikube/certs/cert.pem
I0128 15:41:47.432603  492188 main.go:141] libmachine: Decoding PEM data...
I0128 15:41:47.432608  492188 main.go:141] libmachine: Parsing certificate...
I0128 15:41:47.432614  492188 main.go:141] libmachine: Running pre-create checks...
I0128 15:41:47.432617  492188 main.go:141] libmachine: (minikube) Calling .PreCreateCheck
I0128 15:41:47.432751  492188 main.go:141] libmachine: (minikube) Calling .GetConfigRaw
I0128 15:41:47.432918  492188 main.go:141] libmachine: Creating machine...
I0128 15:41:47.432921  492188 main.go:141] libmachine: (minikube) Calling .Create
I0128 15:41:47.432960  492188 main.go:141] libmachine: (minikube) Creating KVM machine...
I0128 15:41:47.433735  492188 main.go:141] libmachine: (minikube) DBG | found existing default KVM network
I0128 15:41:47.434634  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:47.434572  492291 network.go:209] using free private subnet 192.168.39.0/24: &{IP:192.168.39.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.39.0/24 Gateway:192.168.39.1 ClientMin:192.168.39.2 ClientMax:192.168.39.254 Broadcast:192.168.39.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc000015e70}
I0128 15:41:47.437973  492188 main.go:141] libmachine: (minikube) DBG | trying to create private KVM network mk-minikube 192.168.39.0/24...
I0128 15:41:47.493661  492188 main.go:141] libmachine: (minikube) DBG | private KVM network mk-minikube 192.168.39.0/24 created
I0128 15:41:47.493671  492188 main.go:141] libmachine: (minikube) Setting up store path in /home/ernado/.minikube/machines/minikube ...
I0128 15:41:47.493679  492188 main.go:141] libmachine: (minikube) Building disk image from file:///src/faster/ytst/deploy/minikube-amd64.iso
I0128 15:41:47.493685  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:47.493662  492291 common.go:145] Making disk image using store path: /home/ernado/.minikube
I0128 15:41:47.493755  492188 main.go:141] libmachine: (minikube) Downloading /home/ernado/.minikube/cache/boot2docker.iso from file:///src/faster/ytst/deploy/minikube-amd64.iso...
I0128 15:41:47.734095  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:47.734061  492291 common.go:152] Creating ssh key: /home/ernado/.minikube/machines/minikube/id_rsa...
I0128 15:41:47.774859  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:47.774804  492291 common.go:158] Creating raw disk image: /home/ernado/.minikube/machines/minikube/minikube.rawdisk...
I0128 15:41:47.774869  492188 main.go:141] libmachine: (minikube) DBG | Writing magic tar header
I0128 15:41:47.774876  492188 main.go:141] libmachine: (minikube) DBG | Writing SSH key tar header
I0128 15:41:47.774880  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:47.774868  492291 common.go:172] Fixing permissions on /home/ernado/.minikube/machines/minikube ...
I0128 15:41:47.774926  492188 main.go:141] libmachine: (minikube) DBG | Checking permissions on dir: /home/ernado/.minikube/machines/minikube
I0128 15:41:47.774933  492188 main.go:141] libmachine: (minikube) Setting executable bit set on /home/ernado/.minikube/machines/minikube (perms=drwx------)
I0128 15:41:47.774950  492188 main.go:141] libmachine: (minikube) DBG | Checking permissions on dir: /home/ernado/.minikube/machines
I0128 15:41:47.774953  492188 main.go:141] libmachine: (minikube) DBG | Checking permissions on dir: /home/ernado/.minikube
I0128 15:41:47.774957  492188 main.go:141] libmachine: (minikube) Setting executable bit set on /home/ernado/.minikube/machines (perms=drwxrwxr-x)
I0128 15:41:47.774962  492188 main.go:141] libmachine: (minikube) Setting executable bit set on /home/ernado/.minikube (perms=drwxr-xr-x)
I0128 15:41:47.774968  492188 main.go:141] libmachine: (minikube) DBG | Checking permissions on dir: /home/ernado
I0128 15:41:47.774973  492188 main.go:141] libmachine: (minikube) Setting executable bit set on /home/ernado (perms=drwxr-x--x)
I0128 15:41:47.774987  492188 main.go:141] libmachine: (minikube) Creating domain...
I0128 15:41:47.775029  492188 main.go:141] libmachine: (minikube) DBG | Checking permissions on dir: /home
I0128 15:41:47.775035  492188 main.go:141] libmachine: (minikube) DBG | Skipping /home - not owner
I0128 15:41:47.775782  492188 main.go:141] libmachine: (minikube) define libvirt domain using xml: 
I0128 15:41:47.775785  492188 main.go:141] libmachine: (minikube) <domain type='kvm'>
I0128 15:41:47.775788  492188 main.go:141] libmachine: (minikube)   <name>minikube</name>
I0128 15:41:47.775790  492188 main.go:141] libmachine: (minikube)   <memory unit='MiB'>6000</memory>
I0128 15:41:47.775792  492188 main.go:141] libmachine: (minikube)   <vcpu>2</vcpu>
I0128 15:41:47.775794  492188 main.go:141] libmachine: (minikube)   <features>
I0128 15:41:47.775797  492188 main.go:141] libmachine: (minikube)     <acpi/>
I0128 15:41:47.775799  492188 main.go:141] libmachine: (minikube)     <apic/>
I0128 15:41:47.775801  492188 main.go:141] libmachine: (minikube)     <pae/>
I0128 15:41:47.775803  492188 main.go:141] libmachine: (minikube)     
I0128 15:41:47.775805  492188 main.go:141] libmachine: (minikube)   </features>
I0128 15:41:47.775807  492188 main.go:141] libmachine: (minikube)   <cpu mode='host-passthrough'>
I0128 15:41:47.775812  492188 main.go:141] libmachine: (minikube)   
I0128 15:41:47.775814  492188 main.go:141] libmachine: (minikube)   </cpu>
I0128 15:41:47.775816  492188 main.go:141] libmachine: (minikube)   <os>
I0128 15:41:47.775818  492188 main.go:141] libmachine: (minikube)     <type>hvm</type>
I0128 15:41:47.775820  492188 main.go:141] libmachine: (minikube)     <boot dev='cdrom'/>
I0128 15:41:47.775822  492188 main.go:141] libmachine: (minikube)     <boot dev='hd'/>
I0128 15:41:47.775825  492188 main.go:141] libmachine: (minikube)     <bootmenu enable='no'/>
I0128 15:41:47.775828  492188 main.go:141] libmachine: (minikube)   </os>
I0128 15:41:47.775831  492188 main.go:141] libmachine: (minikube)   <devices>
I0128 15:41:47.775833  492188 main.go:141] libmachine: (minikube)     <disk type='file' device='cdrom'>
I0128 15:41:47.775836  492188 main.go:141] libmachine: (minikube)       <source file='/home/ernado/.minikube/machines/minikube/boot2docker.iso'/>
I0128 15:41:47.775838  492188 main.go:141] libmachine: (minikube)       <target dev='hdc' bus='scsi'/>
I0128 15:41:47.775840  492188 main.go:141] libmachine: (minikube)       <readonly/>
I0128 15:41:47.775842  492188 main.go:141] libmachine: (minikube)     </disk>
I0128 15:41:47.775846  492188 main.go:141] libmachine: (minikube)     <disk type='file' device='disk'>
I0128 15:41:47.775848  492188 main.go:141] libmachine: (minikube)       <driver name='qemu' type='raw' cache='default' io='threads' />
I0128 15:41:47.775852  492188 main.go:141] libmachine: (minikube)       <source file='/home/ernado/.minikube/machines/minikube/minikube.rawdisk'/>
I0128 15:41:47.775855  492188 main.go:141] libmachine: (minikube)       <target dev='hda' bus='virtio'/>
I0128 15:41:47.775859  492188 main.go:141] libmachine: (minikube)     </disk>
I0128 15:41:47.775862  492188 main.go:141] libmachine: (minikube)     <interface type='network'>
I0128 15:41:47.775864  492188 main.go:141] libmachine: (minikube)       <source network='mk-minikube'/>
I0128 15:41:47.775867  492188 main.go:141] libmachine: (minikube)       <model type='virtio'/>
I0128 15:41:47.775869  492188 main.go:141] libmachine: (minikube)     </interface>
I0128 15:41:47.775872  492188 main.go:141] libmachine: (minikube)     <interface type='network'>
I0128 15:41:47.775874  492188 main.go:141] libmachine: (minikube)       <source network='default'/>
I0128 15:41:47.775877  492188 main.go:141] libmachine: (minikube)       <model type='virtio'/>
I0128 15:41:47.775879  492188 main.go:141] libmachine: (minikube)     </interface>
I0128 15:41:47.775881  492188 main.go:141] libmachine: (minikube)     <serial type='pty'>
I0128 15:41:47.775883  492188 main.go:141] libmachine: (minikube)       <target port='0'/>
I0128 15:41:47.775885  492188 main.go:141] libmachine: (minikube)     </serial>
I0128 15:41:47.775888  492188 main.go:141] libmachine: (minikube)     <console type='pty'>
I0128 15:41:47.775890  492188 main.go:141] libmachine: (minikube)       <target type='serial' port='0'/>
I0128 15:41:47.775892  492188 main.go:141] libmachine: (minikube)     </console>
I0128 15:41:47.775894  492188 main.go:141] libmachine: (minikube)     <rng model='virtio'>
I0128 15:41:47.775897  492188 main.go:141] libmachine: (minikube)       <backend model='random'>/dev/random</backend>
I0128 15:41:47.775899  492188 main.go:141] libmachine: (minikube)     </rng>
I0128 15:41:47.775902  492188 main.go:141] libmachine: (minikube)     
I0128 15:41:47.775904  492188 main.go:141] libmachine: (minikube)     
I0128 15:41:47.775906  492188 main.go:141] libmachine: (minikube)   </devices>
I0128 15:41:47.775908  492188 main.go:141] libmachine: (minikube) </domain>
I0128 15:41:47.775911  492188 main.go:141] libmachine: (minikube) 
I0128 15:41:47.780102  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:28:c3:df in network default
I0128 15:41:47.780312  492188 main.go:141] libmachine: (minikube) Ensuring networks are active...
I0128 15:41:47.780377  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:47.780737  492188 main.go:141] libmachine: (minikube) Ensuring network default is active
I0128 15:41:47.780888  492188 main.go:141] libmachine: (minikube) Ensuring network mk-minikube is active
I0128 15:41:47.781125  492188 main.go:141] libmachine: (minikube) Getting domain xml...
I0128 15:41:47.781482  492188 main.go:141] libmachine: (minikube) Creating domain...
I0128 15:41:48.486865  492188 main.go:141] libmachine: (minikube) Waiting to get IP...
I0128 15:41:48.487210  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:48.487359  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:48.487380  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:48.487363  492291 retry.go:31] will retry after 195.430285ms: waiting for machine to come up
I0128 15:41:48.684138  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:48.684453  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:48.684464  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:48.684422  492291 retry.go:31] will retry after 298.15276ms: waiting for machine to come up
I0128 15:41:48.983268  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:48.983460  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:48.983467  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:48.983440  492291 retry.go:31] will retry after 389.789571ms: waiting for machine to come up
I0128 15:41:49.374402  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:49.374566  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:49.374573  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:49.374564  492291 retry.go:31] will retry after 508.813612ms: waiting for machine to come up
I0128 15:41:49.884822  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:49.884996  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:49.885001  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:49.884983  492291 retry.go:31] will retry after 758.086118ms: waiting for machine to come up
I0128 15:41:50.644526  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:50.644687  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:50.644703  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:50.644683  492291 retry.go:31] will retry after 689.522427ms: waiting for machine to come up
I0128 15:41:51.334951  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:51.335206  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:51.335220  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:51.335200  492291 retry.go:31] will retry after 781.867118ms: waiting for machine to come up
I0128 15:41:52.118768  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:52.119039  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:52.119045  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:52.119019  492291 retry.go:31] will retry after 1.138469811s: waiting for machine to come up
I0128 15:41:53.258800  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:53.259061  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:53.259072  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:53.259037  492291 retry.go:31] will retry after 1.254909828s: waiting for machine to come up
I0128 15:41:54.515083  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:54.515347  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:54.515363  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:54.515323  492291 retry.go:31] will retry after 1.627492017s: waiting for machine to come up
I0128 15:41:56.144319  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:56.144560  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:56.144577  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:56.144558  492291 retry.go:31] will retry after 2.525433918s: waiting for machine to come up
I0128 15:41:58.671718  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:41:58.671958  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:41:58.671981  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:41:58.671953  492291 retry.go:31] will retry after 2.818189864s: waiting for machine to come up
I0128 15:42:01.493474  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:01.493713  492188 main.go:141] libmachine: (minikube) DBG | unable to find current IP address of domain minikube in network mk-minikube
I0128 15:42:01.493723  492188 main.go:141] libmachine: (minikube) DBG | I0128 15:42:01.493707  492291 retry.go:31] will retry after 3.061989375s: waiting for machine to come up
I0128 15:42:04.556424  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.556715  492188 main.go:141] libmachine: (minikube) Found IP for machine: 192.168.39.62
I0128 15:42:04.556731  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has current primary IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.556735  492188 main.go:141] libmachine: (minikube) Reserving static IP address...
I0128 15:42:04.556926  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "minikube", mac: "52:54:00:96:58:99", ip: "192.168.39.62"} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:04.556941  492188 main.go:141] libmachine: (minikube) DBG | skip adding static IP to network mk-minikube - found existing host DHCP lease matching {name: "minikube", mac: "52:54:00:96:58:99", ip: "192.168.39.62"}
I0128 15:42:04.556949  492188 main.go:141] libmachine: (minikube) DBG | Getting to WaitForSSH function...
I0128 15:42:04.556953  492188 main.go:141] libmachine: (minikube) Reserved static IP address: 192.168.39.62
I0128 15:42:04.556958  492188 main.go:141] libmachine: (minikube) Waiting for SSH to be available...
I0128 15:42:04.558821  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.559018  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:04.559025  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.559134  492188 main.go:141] libmachine: (minikube) DBG | Using SSH client type: external
I0128 15:42:04.559148  492188 main.go:141] libmachine: (minikube) DBG | Using SSH private key: /home/ernado/.minikube/machines/minikube/id_rsa (-rw-------)
I0128 15:42:04.559165  492188 main.go:141] libmachine: (minikube) DBG | &{[-F /dev/null -o ConnectionAttempts=3 -o ConnectTimeout=10 -o ControlMaster=no -o ControlPath=none -o LogLevel=quiet -o PasswordAuthentication=no -o ServerAliveInterval=60 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null docker@192.168.39.62 -o IdentitiesOnly=yes -i /home/ernado/.minikube/machines/minikube/id_rsa -p 22] /usr/bin/ssh <nil>}
I0128 15:42:04.559173  492188 main.go:141] libmachine: (minikube) DBG | About to run SSH command:
I0128 15:42:04.559178  492188 main.go:141] libmachine: (minikube) DBG | exit 0
I0128 15:42:04.633383  492188 main.go:141] libmachine: (minikube) DBG | SSH cmd err, output: <nil>: 
I0128 15:42:04.633506  492188 main.go:141] libmachine: (minikube) KVM machine creation complete!
I0128 15:42:04.633716  492188 main.go:141] libmachine: (minikube) Calling .GetConfigRaw
I0128 15:42:04.634030  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:42:04.634125  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:42:04.634172  492188 main.go:141] libmachine: Waiting for machine to be running, this may take a few minutes...
I0128 15:42:04.634174  492188 main.go:141] libmachine: (minikube) Calling .GetState
I0128 15:42:04.634784  492188 main.go:141] libmachine: Detecting operating system of created instance...
I0128 15:42:04.634788  492188 main.go:141] libmachine: Waiting for SSH to be available...
I0128 15:42:04.634790  492188 main.go:141] libmachine: Getting to WaitForSSH function...
I0128 15:42:04.634792  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:04.635809  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.635922  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:04.635927  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.635974  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:04.636025  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:04.636069  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:04.636119  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:04.636192  492188 main.go:141] libmachine: Using SSH client type: native
I0128 15:42:04.636376  492188 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80a8e0] 0x80d5c0 <nil>  [] 0s} 192.168.39.62 22 <nil> <nil>}
I0128 15:42:04.636379  492188 main.go:141] libmachine: About to run SSH command:
exit 0
I0128 15:42:04.724781  492188 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0128 15:42:04.724792  492188 main.go:141] libmachine: Detecting the provisioner...
I0128 15:42:04.724796  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:04.725980  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.726097  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:04.726106  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.726160  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:04.726238  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:04.726301  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:04.726351  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:04.726422  492188 main.go:141] libmachine: Using SSH client type: native
I0128 15:42:04.726601  492188 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80a8e0] 0x80d5c0 <nil>  [] 0s} 192.168.39.62 22 <nil> <nil>}
I0128 15:42:04.726604  492188 main.go:141] libmachine: About to run SSH command:
cat /etc/os-release
I0128 15:42:04.817269  492188 main.go:141] libmachine: SSH cmd err, output: <nil>: NAME=Buildroot
VERSION=2021.02.12-1-ge2d517d-dirty
ID=buildroot
VERSION_ID=2021.02.12
PRETTY_NAME="Buildroot 2021.02.12"

I0128 15:42:04.817301  492188 main.go:141] libmachine: found compatible host: buildroot
I0128 15:42:04.817304  492188 main.go:141] libmachine: Provisioning with buildroot...
I0128 15:42:04.817307  492188 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0128 15:42:04.817383  492188 buildroot.go:166] provisioning hostname "minikube"
I0128 15:42:04.817389  492188 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0128 15:42:04.817453  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:04.818739  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.818858  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:04.818865  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.818927  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:04.818979  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:04.819026  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:04.819065  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:04.819135  492188 main.go:141] libmachine: Using SSH client type: native
I0128 15:42:04.819296  492188 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80a8e0] 0x80d5c0 <nil>  [] 0s} 192.168.39.62 22 <nil> <nil>}
I0128 15:42:04.819299  492188 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0128 15:42:04.913098  492188 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0128 15:42:04.913107  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:04.914650  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.914773  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:04.914780  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:04.914839  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:04.914913  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:04.914976  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:04.915030  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:04.915103  492188 main.go:141] libmachine: Using SSH client type: native
I0128 15:42:04.915269  492188 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80a8e0] 0x80d5c0 <nil>  [] 0s} 192.168.39.62 22 <nil> <nil>}
I0128 15:42:04.915275  492188 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0128 15:42:05.005511  492188 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0128 15:42:05.005519  492188 buildroot.go:172] set auth options {CertDir:/home/ernado/.minikube CaCertPath:/home/ernado/.minikube/certs/ca.pem CaPrivateKeyPath:/home/ernado/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/ernado/.minikube/machines/server.pem ServerKeyPath:/home/ernado/.minikube/machines/server-key.pem ClientKeyPath:/home/ernado/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/ernado/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/ernado/.minikube}
I0128 15:42:05.005530  492188 buildroot.go:174] setting up certificates
I0128 15:42:05.005535  492188 provision.go:83] configureAuth start
I0128 15:42:05.005539  492188 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0128 15:42:05.005635  492188 main.go:141] libmachine: (minikube) Calling .GetIP
I0128 15:42:05.006793  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.006900  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.006913  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.006938  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:05.008031  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.008125  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.008139  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.008177  492188 provision.go:138] copyHostCerts
I0128 15:42:05.008205  492188 exec_runner.go:144] found /home/ernado/.minikube/ca.pem, removing ...
I0128 15:42:05.008207  492188 exec_runner.go:203] rm: /home/ernado/.minikube/ca.pem
I0128 15:42:05.008264  492188 exec_runner.go:151] cp: /home/ernado/.minikube/certs/ca.pem --> /home/ernado/.minikube/ca.pem (1078 bytes)
I0128 15:42:05.008320  492188 exec_runner.go:144] found /home/ernado/.minikube/cert.pem, removing ...
I0128 15:42:05.008322  492188 exec_runner.go:203] rm: /home/ernado/.minikube/cert.pem
I0128 15:42:05.008346  492188 exec_runner.go:151] cp: /home/ernado/.minikube/certs/cert.pem --> /home/ernado/.minikube/cert.pem (1123 bytes)
I0128 15:42:05.008380  492188 exec_runner.go:144] found /home/ernado/.minikube/key.pem, removing ...
I0128 15:42:05.008381  492188 exec_runner.go:203] rm: /home/ernado/.minikube/key.pem
I0128 15:42:05.008401  492188 exec_runner.go:151] cp: /home/ernado/.minikube/certs/key.pem --> /home/ernado/.minikube/key.pem (1679 bytes)
I0128 15:42:05.008433  492188 provision.go:112] generating server cert: /home/ernado/.minikube/machines/server.pem ca-key=/home/ernado/.minikube/certs/ca.pem private-key=/home/ernado/.minikube/certs/ca-key.pem org=ernado.minikube san=[192.168.39.62 192.168.39.62 localhost 127.0.0.1 minikube minikube]
I0128 15:42:05.061387  492188 provision.go:172] copyRemoteCerts
I0128 15:42:05.061425  492188 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0128 15:42:05.061432  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:05.062794  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.062905  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.062911  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.062984  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:05.063160  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.063218  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:05.063267  492188 sshutil.go:53] new ssh client: &{IP:192.168.39.62 Port:22 SSHKeyPath:/home/ernado/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:42:05.130575  492188 ssh_runner.go:362] scp /home/ernado/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0128 15:42:05.136632  492188 ssh_runner.go:362] scp /home/ernado/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0128 15:42:05.142461  492188 ssh_runner.go:362] scp /home/ernado/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I0128 15:42:05.148083  492188 provision.go:86] duration metric: configureAuth took 142.543657ms
I0128 15:42:05.148091  492188 buildroot.go:189] setting minikube options for container-runtime
I0128 15:42:05.148194  492188 config.go:182] Loaded profile config "minikube": Driver=kvm2, ContainerRuntime=porto, KubernetesVersion=v1.28.4
I0128 15:42:05.148204  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:42:05.148365  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:05.149737  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.149921  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.149938  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.149964  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:05.150045  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.150113  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.150170  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:05.150253  492188 main.go:141] libmachine: Using SSH client type: native
I0128 15:42:05.150428  492188 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80a8e0] 0x80d5c0 <nil>  [] 0s} 192.168.39.62 22 <nil> <nil>}
I0128 15:42:05.150431  492188 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0128 15:42:05.237261  492188 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0128 15:42:05.237269  492188 buildroot.go:70] root file system type: tmpfs
I0128 15:42:05.237323  492188 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0128 15:42:05.237329  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:05.238852  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.238998  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.239004  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.239078  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:05.239219  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.239285  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.239346  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:05.239428  492188 main.go:141] libmachine: Using SSH client type: native
I0128 15:42:05.239607  492188 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80a8e0] 0x80d5c0 <nil>  [] 0s} 192.168.39.62 22 <nil> <nil>}
I0128 15:42:05.239634  492188 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=kvm2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0128 15:42:05.328362  492188 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=kvm2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0128 15:42:05.328372  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:05.329635  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.329748  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.329753  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.329830  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:05.329897  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.329948  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.329994  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:05.330061  492188 main.go:141] libmachine: Using SSH client type: native
I0128 15:42:05.330224  492188 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80a8e0] 0x80d5c0 <nil>  [] 0s} 192.168.39.62 22 <nil> <nil>}
I0128 15:42:05.330229  492188 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0128 15:42:05.669974  492188 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /usr/lib/systemd/system/docker.service.

I0128 15:42:05.669986  492188 main.go:141] libmachine: Checking connection to Docker...
I0128 15:42:05.669993  492188 main.go:141] libmachine: (minikube) Calling .GetURL
I0128 15:42:05.670739  492188 main.go:141] libmachine: (minikube) DBG | Using libvirt version 8000000
I0128 15:42:05.671841  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.671981  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.671985  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.672062  492188 main.go:141] libmachine: Docker is up and running!
I0128 15:42:05.672065  492188 main.go:141] libmachine: Reticulating splines...
I0128 15:42:05.672069  492188 client.go:171] LocalClient.Create took 18.239527793s
I0128 15:42:05.672079  492188 start.go:167] duration metric: libmachine.API.Create for "minikube" took 18.239548281s
I0128 15:42:05.672082  492188 start.go:300] post-start starting for "minikube" (driver="kvm2")
I0128 15:42:05.672088  492188 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0128 15:42:05.672094  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:42:05.672244  492188 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0128 15:42:05.672252  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:05.673145  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.673254  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.673261  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.673302  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:05.673350  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.673398  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:05.673443  492188 sshutil.go:53] new ssh client: &{IP:192.168.39.62 Port:22 SSHKeyPath:/home/ernado/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:42:05.742748  492188 ssh_runner.go:195] Run: cat /etc/os-release
I0128 15:42:05.743774  492188 info.go:137] Remote host: Buildroot 2021.02.12
I0128 15:42:05.743780  492188 filesync.go:126] Scanning /home/ernado/.minikube/addons for local assets ...
I0128 15:42:05.743814  492188 filesync.go:126] Scanning /home/ernado/.minikube/files for local assets ...
I0128 15:42:05.743825  492188 start.go:303] post-start completed in 71.740334ms
I0128 15:42:05.743835  492188 main.go:141] libmachine: (minikube) Calling .GetConfigRaw
I0128 15:42:05.744076  492188 main.go:141] libmachine: (minikube) Calling .GetIP
I0128 15:42:05.745374  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.745490  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.745496  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.745602  492188 profile.go:148] Saving config to /home/ernado/.minikube/profiles/minikube/config.json ...
I0128 15:42:05.745717  492188 start.go:128] duration metric: createHost completed in 18.325778165s
I0128 15:42:05.745726  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:05.746676  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.746799  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.746811  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.746838  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:05.746887  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.746940  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.746984  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:05.747046  492188 main.go:141] libmachine: Using SSH client type: native
I0128 15:42:05.747204  492188 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80a8e0] 0x80d5c0 <nil>  [] 0s} 192.168.39.62 22 <nil> <nil>}
I0128 15:42:05.747207  492188 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I0128 15:42:05.832912  492188 main.go:141] libmachine: SSH cmd err, output: <nil>: 1706445725.826555596

I0128 15:42:05.832918  492188 fix.go:206] guest clock: 1706445725.826555596
I0128 15:42:05.832922  492188 fix.go:219] Guest: 2024-01-28 15:42:05.826555596 +0300 MSK Remote: 2024-01-28 15:42:05.745719314 +0300 MSK m=+18.669586715 (delta=80.836282ms)
I0128 15:42:05.832939  492188 fix.go:190] guest clock delta is within tolerance: 80.836282ms
I0128 15:42:05.832941  492188 start.go:83] releasing machines lock for "minikube", held for 18.413046346s
I0128 15:42:05.832951  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:42:05.833079  492188 main.go:141] libmachine: (minikube) Calling .GetIP
I0128 15:42:05.834248  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.834368  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.834375  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.834410  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:42:05.834604  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:42:05.834672  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:42:05.834724  492188 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0128 15:42:05.834737  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:05.834766  492188 ssh_runner.go:195] Run: cat /version.json
I0128 15:42:05.834772  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:05.835764  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.835873  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.835879  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.835886  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.835935  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:05.836001  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.836053  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:05.836079  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:05.836092  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:05.836098  492188 sshutil.go:53] new ssh client: &{IP:192.168.39.62 Port:22 SSHKeyPath:/home/ernado/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:42:05.836127  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:05.836163  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:05.836207  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:05.836248  492188 sshutil.go:53] new ssh client: &{IP:192.168.39.62 Port:22 SSHKeyPath:/home/ernado/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:42:05.966693  492188 ssh_runner.go:195] Run: systemctl --version
I0128 15:42:05.968213  492188 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0128 15:42:05.969410  492188 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0128 15:42:05.969450  492188 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0128 15:42:05.973187  492188 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0128 15:42:05.973193  492188 start.go:475] detecting cgroup driver to use...
I0128 15:42:05.973234  492188 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0128 15:42:05.976975  492188 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0128 15:42:05.980229  492188 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0128 15:42:05.993178  492188 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0128 15:42:05.996371  492188 docker.go:203] disabling cri-docker service (if available) ...
I0128 15:42:05.996413  492188 ssh_runner.go:195] Run: sudo systemctl stop -f cri-docker.socket
I0128 15:42:05.999745  492188 ssh_runner.go:195] Run: sudo systemctl stop -f cri-docker.service
I0128 15:42:06.003009  492188 ssh_runner.go:195] Run: sudo systemctl disable cri-docker.socket
I0128 15:42:06.043046  492188 ssh_runner.go:195] Run: sudo systemctl mask cri-docker.service
I0128 15:42:06.080597  492188 docker.go:219] disabling docker service ...
I0128 15:42:06.080631  492188 ssh_runner.go:195] Run: sudo systemctl stop -f docker.socket
I0128 15:42:07.087771  492188 ssh_runner.go:235] Completed: sudo systemctl stop -f docker.socket: (1.007124512s)
I0128 15:42:07.087832  492188 ssh_runner.go:195] Run: sudo systemctl stop -f docker.service
I0128 15:42:07.092347  492188 ssh_runner.go:195] Run: sudo systemctl disable docker.socket
I0128 15:42:07.137222  492188 ssh_runner.go:195] Run: sudo systemctl mask docker.service
I0128 15:42:07.182214  492188 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I0128 15:42:07.185772  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/portoshim.sock
" | sudo tee /etc/crictl.yaml"
I0128 15:42:07.190091  492188 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0128 15:42:07.192227  492188 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0128 15:42:07.194327  492188 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 15:42:07.235393  492188 ssh_runner.go:195] Run: sudo systemctl restart porto
I0128 15:42:07.253623  492188 cri.go:205] Pulling image: registry.k8s.io/pause:3.7
I0128 15:42:07.253697  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:07.254854  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl pull registry.k8s.io/pause:3.7
I0128 15:42:08.802523  492188 ssh_runner.go:235] Completed: sudo /usr/bin/crictl pull registry.k8s.io/pause:3.7: (1.547654276s)
I0128 15:42:08.802535  492188 start.go:522] Will wait 60s for socket path /run/portoshim.sock
I0128 15:42:08.802589  492188 ssh_runner.go:195] Run: stat /run/portoshim.sock
I0128 15:42:08.804065  492188 start.go:543] Will wait 60s for crictl version
I0128 15:42:08.804098  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:08.805033  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0128 15:42:08.815059  492188 start.go:559] Version:  v1
RuntimeName:  porto
RuntimeVersion:  5.3.30-alpha.9
RuntimeApiVersion:  5.3.30-alpha.9
I0128 15:42:08.815133  492188 ssh_runner.go:195] Run: portod version
I0128 15:42:08.818174  492188 ssh_runner.go:195] Run: portod version
I0128 15:42:08.823449  492188 out.go:177] üéÅ  Preparing Kubernetes v1.28.4 on porto 5.3.30-alpha.9 ...
I0128 15:42:08.824383  492188 main.go:141] libmachine: (minikube) Calling .GetIP
I0128 15:42:08.825794  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:08.825924  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:08.825930  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:08.826034  492188 ssh_runner.go:195] Run: grep 192.168.39.1	host.minikube.internal$ /etc/hosts
I0128 15:42:08.826990  492188 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.39.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0128 15:42:08.830255  492188 ssh_runner.go:195] Run: sudo crictl images --output json
I0128 15:42:08.840461  492188 porto.go:307] couldn't find preloaded image for "registry.k8s.io/kube-apiserver:v1.28.4". assuming images are not preloaded.
I0128 15:42:08.840466  492188 cri.go:205] Pulling image: registry.k8s.io/kube-apiserver:v1.28.4
I0128 15:42:08.840496  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:08.841444  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl pull registry.k8s.io/kube-apiserver:v1.28.4
I0128 15:42:11.811855  492188 ssh_runner.go:235] Completed: sudo /usr/bin/crictl pull registry.k8s.io/kube-apiserver:v1.28.4: (2.970395904s)
I0128 15:42:11.811866  492188 cri.go:205] Pulling image: registry.k8s.io/kube-controller-manager:v1.28.4
I0128 15:42:11.811916  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:11.813070  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl pull registry.k8s.io/kube-controller-manager:v1.28.4
I0128 15:42:14.953704  492188 ssh_runner.go:235] Completed: sudo /usr/bin/crictl pull registry.k8s.io/kube-controller-manager:v1.28.4: (3.140621227s)
I0128 15:42:14.953713  492188 cri.go:205] Pulling image: registry.k8s.io/kube-scheduler:v1.28.4
I0128 15:42:14.953754  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:14.955056  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl pull registry.k8s.io/kube-scheduler:v1.28.4
I0128 15:42:17.222367  492188 ssh_runner.go:235] Completed: sudo /usr/bin/crictl pull registry.k8s.io/kube-scheduler:v1.28.4: (2.267296229s)
I0128 15:42:17.222378  492188 cri.go:205] Pulling image: registry.k8s.io/kube-proxy:v1.28.4
I0128 15:42:17.222432  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:17.224061  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl pull registry.k8s.io/kube-proxy:v1.28.4
I0128 15:42:19.791498  492188 ssh_runner.go:235] Completed: sudo /usr/bin/crictl pull registry.k8s.io/kube-proxy:v1.28.4: (2.567424916s)
I0128 15:42:19.791508  492188 cri.go:205] Pulling image: registry.k8s.io/pause:3.9
I0128 15:42:19.791552  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:19.792914  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl pull registry.k8s.io/pause:3.9
I0128 15:42:21.258114  492188 ssh_runner.go:235] Completed: sudo /usr/bin/crictl pull registry.k8s.io/pause:3.9: (1.465183953s)
I0128 15:42:21.258128  492188 cri.go:205] Pulling image: registry.k8s.io/pause:3.7
I0128 15:42:21.258178  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:21.259713  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl pull registry.k8s.io/pause:3.7
I0128 15:42:22.141142  492188 cri.go:205] Pulling image: registry.k8s.io/etcd:3.5.9-0
I0128 15:42:22.141201  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:22.142533  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl pull registry.k8s.io/etcd:3.5.9-0
I0128 15:42:28.035380  492188 ssh_runner.go:235] Completed: sudo /usr/bin/crictl pull registry.k8s.io/etcd:3.5.9-0: (5.892831689s)
I0128 15:42:28.035394  492188 cri.go:205] Pulling image: registry.k8s.io/coredns/coredns:v1.10.1
I0128 15:42:28.035445  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:28.036652  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl pull registry.k8s.io/coredns/coredns:v1.10.1
I0128 15:42:29.997369  492188 ssh_runner.go:235] Completed: sudo /usr/bin/crictl pull registry.k8s.io/coredns/coredns:v1.10.1: (1.960702879s)
I0128 15:42:29.997379  492188 cri.go:205] Pulling image: gcr.io/k8s-minikube/storage-provisioner:v5
I0128 15:42:29.997422  492188 ssh_runner.go:195] Run: which crictl
I0128 15:42:29.998961  492188 ssh_runner.go:195] Run: sudo /usr/bin/crictl pull gcr.io/k8s-minikube/storage-provisioner:v5
I0128 15:42:32.558981  492188 ssh_runner.go:235] Completed: sudo /usr/bin/crictl pull gcr.io/k8s-minikube/storage-provisioner:v5: (2.560004265s)
I0128 15:42:32.559043  492188 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 15:42:32.598613  492188 ssh_runner.go:195] Run: sudo systemctl restart porto
I0128 15:42:32.626563  492188 cni.go:84] Creating CNI manager for "cilium"
I0128 15:42:32.626577  492188 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0128 15:42:32.626589  492188 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.39.62 APIServerPort:8443 KubernetesVersion:v1.28.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/run/portoshim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.39.62"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.39.62 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0128 15:42:32.626677  492188 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.39.62
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///run/portoshim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.39.62
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.39.62"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0128 15:42:32.626722  492188 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///run/portoshim.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.39.62

[Install]
 config:
{KubernetesVersion:v1.28.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:porto CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:false EnableDefaultCNI:false CNI:cilium NodeIP: NodePort:8443 NodeName:}
I0128 15:42:32.626771  492188 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.4
I0128 15:42:32.630328  492188 binaries.go:47] Didn't find k8s binaries: sudo ls /var/lib/minikube/binaries/v1.28.4: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/binaries/v1.28.4': No such file or directory

Initiating transfer...
I0128 15:42:32.630370  492188 ssh_runner.go:195] Run: sudo mkdir -p /var/lib/minikube/binaries/v1.28.4
I0128 15:42:32.632907  492188 binary.go:76] Not caching binary, using https://dl.k8s.io/release/v1.28.4/bin/linux/amd64/kubectl?checksum=file:https://dl.k8s.io/release/v1.28.4/bin/linux/amd64/kubectl.sha256
I0128 15:42:32.632929  492188 binary.go:76] Not caching binary, using https://dl.k8s.io/release/v1.28.4/bin/linux/amd64/kubeadm?checksum=file:https://dl.k8s.io/release/v1.28.4/bin/linux/amd64/kubeadm.sha256
I0128 15:42:32.632961  492188 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.28.4/kubectl
I0128 15:42:32.632971  492188 binary.go:76] Not caching binary, using https://dl.k8s.io/release/v1.28.4/bin/linux/amd64/kubelet?checksum=file:https://dl.k8s.io/release/v1.28.4/bin/linux/amd64/kubelet.sha256
I0128 15:42:32.632988  492188 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.28.4/kubeadm
I0128 15:42:32.632997  492188 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0128 15:42:32.640477  492188 ssh_runner.go:195] Run: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.28.4/kubelet
I0128 15:42:32.641931  492188 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.28.4/kubelet: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.28.4/kubelet: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/binaries/v1.28.4/kubelet': No such file or directory
I0128 15:42:32.641930  492188 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.28.4/kubeadm: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.28.4/kubeadm: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/binaries/v1.28.4/kubeadm': No such file or directory
I0128 15:42:32.641936  492188 ssh_runner.go:362] scp /home/ernado/.minikube/cache/linux/amd64/v1.28.4/kubelet --> /var/lib/minikube/binaries/v1.28.4/kubelet (110850048 bytes)
I0128 15:42:32.641941  492188 ssh_runner.go:362] scp /home/ernado/.minikube/cache/linux/amd64/v1.28.4/kubeadm --> /var/lib/minikube/binaries/v1.28.4/kubeadm (49102848 bytes)
I0128 15:42:32.641979  492188 ssh_runner.go:352] existence check for /var/lib/minikube/binaries/v1.28.4/kubectl: stat -c "%!s(MISSING) %!y(MISSING)" /var/lib/minikube/binaries/v1.28.4/kubectl: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/binaries/v1.28.4/kubectl': No such file or directory
I0128 15:42:32.641982  492188 ssh_runner.go:362] scp /home/ernado/.minikube/cache/linux/amd64/v1.28.4/kubectl --> /var/lib/minikube/binaries/v1.28.4/kubectl (49885184 bytes)
I0128 15:42:32.862337  492188 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0128 15:42:32.864718  492188 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (364 bytes)
I0128 15:42:32.868807  492188 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0128 15:42:32.872813  492188 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2087 bytes)
I0128 15:42:32.876868  492188 ssh_runner.go:195] Run: grep 192.168.39.62	control-plane.minikube.internal$ /etc/hosts
I0128 15:42:32.877764  492188 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.39.62	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0128 15:42:32.880837  492188 certs.go:56] Setting up /home/ernado/.minikube/profiles/minikube for IP: 192.168.39.62
I0128 15:42:32.880846  492188 certs.go:190] acquiring lock for shared ca certs: {Name:mk0b377783b1a9f8ed535762ad4120f5078d9c11 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:42:32.880919  492188 certs.go:199] skipping minikubeCA CA generation: /home/ernado/.minikube/ca.key
I0128 15:42:32.880939  492188 certs.go:199] skipping proxyClientCA CA generation: /home/ernado/.minikube/proxy-client-ca.key
I0128 15:42:32.880966  492188 certs.go:319] generating minikube-user signed cert: /home/ernado/.minikube/profiles/minikube/client.key
I0128 15:42:32.880973  492188 crypto.go:68] Generating cert /home/ernado/.minikube/profiles/minikube/client.crt with IP's: []
I0128 15:42:32.964071  492188 crypto.go:156] Writing cert to /home/ernado/.minikube/profiles/minikube/client.crt ...
I0128 15:42:32.964075  492188 lock.go:35] WriteFile acquiring /home/ernado/.minikube/profiles/minikube/client.crt: {Name:mk1f0a884efac09bc6768940e5bac2d5560519a6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:42:32.964143  492188 crypto.go:164] Writing key to /home/ernado/.minikube/profiles/minikube/client.key ...
I0128 15:42:32.964144  492188 lock.go:35] WriteFile acquiring /home/ernado/.minikube/profiles/minikube/client.key: {Name:mkd6d76e7772773b339deba5ba1d61b24d641fbb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:42:32.964182  492188 certs.go:319] generating minikube signed cert: /home/ernado/.minikube/profiles/minikube/apiserver.key.7d2852c5
I0128 15:42:32.964189  492188 crypto.go:68] Generating cert /home/ernado/.minikube/profiles/minikube/apiserver.crt.7d2852c5 with IP's: [192.168.39.62 10.96.0.1 127.0.0.1 10.0.0.1]
I0128 15:42:32.993570  492188 crypto.go:156] Writing cert to /home/ernado/.minikube/profiles/minikube/apiserver.crt.7d2852c5 ...
I0128 15:42:32.993573  492188 lock.go:35] WriteFile acquiring /home/ernado/.minikube/profiles/minikube/apiserver.crt.7d2852c5: {Name:mkba8f1e05dcb67fc7ee7f5e662d20328eeb426a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:42:32.993649  492188 crypto.go:164] Writing key to /home/ernado/.minikube/profiles/minikube/apiserver.key.7d2852c5 ...
I0128 15:42:32.993658  492188 lock.go:35] WriteFile acquiring /home/ernado/.minikube/profiles/minikube/apiserver.key.7d2852c5: {Name:mk41fe56c734810a286375f8bee59f3db9a7fd45 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:42:32.993706  492188 certs.go:337] copying /home/ernado/.minikube/profiles/minikube/apiserver.crt.7d2852c5 -> /home/ernado/.minikube/profiles/minikube/apiserver.crt
I0128 15:42:32.993752  492188 certs.go:341] copying /home/ernado/.minikube/profiles/minikube/apiserver.key.7d2852c5 -> /home/ernado/.minikube/profiles/minikube/apiserver.key
I0128 15:42:32.993789  492188 certs.go:319] generating aggregator signed cert: /home/ernado/.minikube/profiles/minikube/proxy-client.key
I0128 15:42:32.993795  492188 crypto.go:68] Generating cert /home/ernado/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0128 15:42:33.079873  492188 crypto.go:156] Writing cert to /home/ernado/.minikube/profiles/minikube/proxy-client.crt ...
I0128 15:42:33.079877  492188 lock.go:35] WriteFile acquiring /home/ernado/.minikube/profiles/minikube/proxy-client.crt: {Name:mk3ebe4072eedb1d8e230c6242714aad9bde8889 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:42:33.079914  492188 crypto.go:164] Writing key to /home/ernado/.minikube/profiles/minikube/proxy-client.key ...
I0128 15:42:33.079916  492188 lock.go:35] WriteFile acquiring /home/ernado/.minikube/profiles/minikube/proxy-client.key: {Name:mka72fbdd529ca06a2a05d19ccf0c81abd7def02 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:42:33.080001  492188 certs.go:437] found cert: /home/ernado/.minikube/certs/home/ernado/.minikube/certs/ca-key.pem (1679 bytes)
I0128 15:42:33.080013  492188 certs.go:437] found cert: /home/ernado/.minikube/certs/home/ernado/.minikube/certs/ca.pem (1078 bytes)
I0128 15:42:33.080024  492188 certs.go:437] found cert: /home/ernado/.minikube/certs/home/ernado/.minikube/certs/cert.pem (1123 bytes)
I0128 15:42:33.080034  492188 certs.go:437] found cert: /home/ernado/.minikube/certs/home/ernado/.minikube/certs/key.pem (1679 bytes)
I0128 15:42:33.080307  492188 ssh_runner.go:362] scp /home/ernado/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0128 15:42:33.086059  492188 ssh_runner.go:362] scp /home/ernado/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0128 15:42:33.091493  492188 ssh_runner.go:362] scp /home/ernado/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0128 15:42:33.096851  492188 ssh_runner.go:362] scp /home/ernado/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0128 15:42:33.102155  492188 ssh_runner.go:362] scp /home/ernado/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0128 15:42:33.107531  492188 ssh_runner.go:362] scp /home/ernado/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0128 15:42:33.112866  492188 ssh_runner.go:362] scp /home/ernado/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0128 15:42:33.118165  492188 ssh_runner.go:362] scp /home/ernado/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0128 15:42:33.123411  492188 ssh_runner.go:362] scp /home/ernado/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0128 15:42:33.128689  492188 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0128 15:42:33.132497  492188 ssh_runner.go:195] Run: openssl version
I0128 15:42:33.133812  492188 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0128 15:42:33.136103  492188 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0128 15:42:33.137222  492188 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 May  7  2023 /usr/share/ca-certificates/minikubeCA.pem
I0128 15:42:33.137246  492188 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0128 15:42:33.138586  492188 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0128 15:42:33.140856  492188 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0128 15:42:33.141734  492188 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0128 15:42:33.141749  492188 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:file:///src/faster/ytst/deploy/minikube-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase-builds:v0.0.42-1704751654-17830@sha256:cabd32f8d9e8d804966eb117ed5366660f6363a4d1415f0b5480de6e396be617 Memory:6000 CPUs:2 DiskSize:20000 VMDriver: Driver:kvm2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:porto CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:false EnableDefaultCNI:false CNI:cilium NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.39.62 Port:8443 KubernetesVersion:v1.28.4 ContainerRuntime:porto ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ernado:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0128 15:42:33.141776  492188 cri.go:54] listing CRI containers in root : {State:paused Name: Namespaces:[kube-system]}
I0128 15:42:33.141801  492188 ssh_runner.go:195] Run: sudo -s eval "crictl ps -a --quiet --label io.kubernetes.pod.namespace=kube-system"
I0128 15:42:33.153187  492188 cri.go:89] found id: ""
I0128 15:42:33.153221  492188 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0128 15:42:33.155451  492188 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0128 15:42:33.157643  492188 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0128 15:42:33.159773  492188 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0128 15:42:33.159795  492188 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.4:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem"
I0128 15:42:33.175142  492188 kubeadm.go:322] [init] Using Kubernetes version: v1.28.4
I0128 15:42:33.175166  492188 kubeadm.go:322] [preflight] Running pre-flight checks
I0128 15:42:33.214366  492188 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0128 15:42:33.214397  492188 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0128 15:42:33.214445  492188 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0128 15:42:33.288126  492188 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0128 15:42:33.290742  492188 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0128 15:42:33.290783  492188 kubeadm.go:322] [certs] Using existing ca certificate authority
I0128 15:42:33.290813  492188 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I0128 15:42:33.361856  492188 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I0128 15:42:33.422284  492188 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I0128 15:42:33.479529  492188 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I0128 15:42:33.534786  492188 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I0128 15:42:33.579954  492188 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I0128 15:42:33.580018  492188 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.39.62 127.0.0.1 ::1]
I0128 15:42:33.675940  492188 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I0128 15:42:33.676022  492188 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.39.62 127.0.0.1 ::1]
I0128 15:42:33.791584  492188 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I0128 15:42:33.825341  492188 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I0128 15:42:33.871628  492188 kubeadm.go:322] [certs] Generating "sa" key and public key
I0128 15:42:33.871664  492188 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0128 15:42:33.946935  492188 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I0128 15:42:33.979137  492188 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0128 15:42:34.025704  492188 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0128 15:42:34.103667  492188 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0128 15:42:34.103905  492188 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0128 15:42:34.104863  492188 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0128 15:42:34.107449  492188 out.go:204]     ‚ñ™ Booting up control plane ...
I0128 15:42:34.107498  492188 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0128 15:42:34.107522  492188 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0128 15:42:34.107543  492188 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0128 15:42:34.111675  492188 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0128 15:42:34.111986  492188 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0128 15:42:34.112024  492188 kubeadm.go:322] [kubelet-start] Starting the kubelet
I0128 15:42:34.155008  492188 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0128 15:42:37.656750  492188 kubeadm.go:322] [apiclient] All control plane components are healthy after 3.501419 seconds
I0128 15:42:37.656813  492188 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0128 15:42:37.669617  492188 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0128 15:42:38.184256  492188 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I0128 15:42:38.184316  492188 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0128 15:42:38.689991  492188 kubeadm.go:322] [bootstrap-token] Using token: 6gk3ki.0jjtsbmjbp0i1v3r
I0128 15:42:38.690952  492188 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I0128 15:42:38.691009  492188 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0128 15:42:38.693087  492188 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0128 15:42:38.696286  492188 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0128 15:42:38.699370  492188 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0128 15:42:38.702346  492188 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0128 15:42:38.703717  492188 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0128 15:42:38.710087  492188 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0128 15:42:38.826301  492188 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I0128 15:42:39.096198  492188 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I0128 15:42:39.096206  492188 kubeadm.go:322] 
I0128 15:42:39.096237  492188 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I0128 15:42:39.096239  492188 kubeadm.go:322] 
I0128 15:42:39.096277  492188 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I0128 15:42:39.096279  492188 kubeadm.go:322] 
I0128 15:42:39.096317  492188 kubeadm.go:322]   mkdir -p $HOME/.kube
I0128 15:42:39.096359  492188 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0128 15:42:39.096394  492188 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0128 15:42:39.096397  492188 kubeadm.go:322] 
I0128 15:42:39.096423  492188 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I0128 15:42:39.096425  492188 kubeadm.go:322] 
I0128 15:42:39.096450  492188 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0128 15:42:39.096452  492188 kubeadm.go:322] 
I0128 15:42:39.096496  492188 kubeadm.go:322] You should now deploy a pod network to the cluster.
I0128 15:42:39.096533  492188 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0128 15:42:39.096574  492188 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0128 15:42:39.096576  492188 kubeadm.go:322] 
I0128 15:42:39.096630  492188 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I0128 15:42:39.096657  492188 kubeadm.go:322] and service account keys on each node and then running the following as root:
I0128 15:42:39.096668  492188 kubeadm.go:322] 
I0128 15:42:39.096695  492188 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token 6gk3ki.0jjtsbmjbp0i1v3r \
I0128 15:42:39.096729  492188 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:a3d86a99a8b149dc2d2a5c9227316b05ba760798be7c891cf30eeb9ed675c0ea \
I0128 15:42:39.096736  492188 kubeadm.go:322] 	--control-plane 
I0128 15:42:39.096737  492188 kubeadm.go:322] 
I0128 15:42:39.096765  492188 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I0128 15:42:39.096767  492188 kubeadm.go:322] 
I0128 15:42:39.096792  492188 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token 6gk3ki.0jjtsbmjbp0i1v3r \
I0128 15:42:39.096827  492188 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:a3d86a99a8b149dc2d2a5c9227316b05ba760798be7c891cf30eeb9ed675c0ea 
I0128 15:42:39.098065  492188 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0128 15:42:39.098073  492188 cni.go:84] Creating CNI manager for "cilium"
I0128 15:42:39.099165  492188 out.go:177] üîó  Configuring Cilium (Container Networking Interface) ...
I0128 15:42:39.100133  492188 ssh_runner.go:195] Run: sudo /bin/bash -c "grep 'bpffs /sys/fs/bpf' /proc/mounts || sudo mount bpffs -t bpf /sys/fs/bpf"
I0128 15:42:39.105525  492188 cilium.go:832] Using pod CIDR: 10.244.0.0/16
I0128 15:42:39.105528  492188 cilium.go:843] cilium options: {PodSubnet:10.244.0.0/16}
I0128 15:42:39.105552  492188 cilium.go:847] cilium config:
---
# Source: cilium/templates/cilium-agent-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium
  namespace: kube-system
---
# Source: cilium/templates/cilium-operator-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cilium-operator
  namespace: kube-system
---
# Source: cilium/templates/cilium-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cilium-config
  namespace: kube-system
data:

  # Identity allocation mode selects how identities are shared between cilium
  # nodes by setting how they are stored. The options are "crd" or "kvstore".
  # - "crd" stores identities in kubernetes as CRDs (custom resource definition).
  #   These can be queried with:
  #     kubectl get ciliumid
  # - "kvstore" stores identities in a kvstore, etcd or consul, that is
  #   configured below. Cilium versions before 1.6 supported only the kvstore
  #   backend. Upgrades from these older cilium versions should continue using
  #   the kvstore by commenting out the identity-allocation-mode below, or
  #   setting it to "kvstore".
  identity-allocation-mode: crd
  cilium-endpoint-gc-interval: "5m0s"

  # If you want to run cilium in debug mode change this value to true
  debug: "false"
  # The agent can be put into the following three policy enforcement modes
  # default, always and never.
  # https://docs.cilium.io/en/latest/policy/intro/#policy-enforcement-modes
  enable-policy: "default"

  # Enable IPv4 addressing. If enabled, all endpoints are allocated an IPv4
  # address.
  enable-ipv4: "true"

  # Enable IPv6 addressing. If enabled, all endpoints are allocated an IPv6
  # address.
  enable-ipv6: "false"
  # Users who wish to specify their own custom CNI configuration file must set
  # custom-cni-conf to "true", otherwise Cilium may overwrite the configuration.
  custom-cni-conf: "false"
  enable-bpf-clock-probe: "true"
  # If you want cilium monitor to aggregate tracing for packets, set this level
  # to "low", "medium", or "maximum". The higher the level, the less packets
  # that will be seen in monitor output.
  monitor-aggregation: medium

  # The monitor aggregation interval governs the typical time between monitor
  # notification events for each allowed connection.
  #
  # Only effective when monitor aggregation is set to "medium" or higher.
  monitor-aggregation-interval: 5s

  # The monitor aggregation flags determine which TCP flags which, upon the
  # first observation, cause monitor notifications to be generated.
  #
  # Only effective when monitor aggregation is set to "medium" or higher.
  monitor-aggregation-flags: all
  # Specifies the ratio (0.0-1.0) of total system memory to use for dynamic
  # sizing of the TCP CT, non-TCP CT, NAT and policy BPF maps.
  bpf-map-dynamic-size-ratio: "0.0025"
  # bpf-policy-map-max specifies the maximum number of entries in endpoint
  # policy map (per endpoint)
  bpf-policy-map-max: "16384"
  # bpf-lb-map-max specifies the maximum number of entries in bpf lb service,
  # backend and affinity maps.
  bpf-lb-map-max: "65536"
  # Pre-allocation of map entries allows per-packet latency to be reduced, at
  # the expense of up-front memory allocation for the entries in the maps. The
  # default value below will minimize memory usage in the default installation;
  # users who are sensitive to latency may consider setting this to "true".
  #
  # This option was introduced in Cilium 1.4. Cilium 1.3 and earlier ignore
  # this option and behave as though it is set to "true".
  #
  # If this value is modified, then during the next Cilium startup the restore
  # of existing endpoints and tracking of ongoing connections may be disrupted.
  # As a result, reply packets may be dropped and the load-balancing decisions
  # for established connections may change.
  #
  # If this option is set to "false" during an upgrade from 1.3 or earlier to
  # 1.4 or later, then it may cause one-time disruptions during the upgrade.
  preallocate-bpf-maps: "false"

  # Regular expression matching compatible Istio sidecar istio-proxy
  # container image names
  sidecar-istio-proxy-image: "cilium/istio_proxy"

  # Name of the cluster. Only relevant when building a mesh of clusters.
  cluster-name: cluster
  # Unique ID of the cluster. Must be unique across all conneted clusters and
  # in the range of 1 and 255. Only relevant when building a mesh of clusters.
  cluster-id: "1"

  # Encapsulation mode for communication between nodes
  # Possible values:
  #   - disabled
  #   - vxlan (default)
  #   - geneve
  tunnel: vxlan
  # Enables L7 proxy for L7 policy enforcement and visibility
  enable-l7-proxy: "true"

  # wait-bpf-mount makes init container wait until bpf filesystem is mounted
  wait-bpf-mount: "false"

  masquerade: "true"
  enable-bpf-masquerade: "true"

  enable-xt-socket-fallback: "true"
  install-iptables-rules: "true"

  auto-direct-node-routes: "false"
  enable-bandwidth-manager: "false"
  enable-local-redirect-policy: "false"
  kube-proxy-replacement:  "probe"
  kube-proxy-replacement-healthz-bind-address: ""
  enable-health-check-nodeport: "true"
  node-port-bind-protection: "true"
  enable-auto-protect-node-port-range: "true"
  enable-session-affinity: "true"
  k8s-require-ipv4-pod-cidr: "true"
  k8s-require-ipv6-pod-cidr: "false"
  enable-endpoint-health-checking: "true"
  enable-health-checking: "true"
  enable-well-known-identities: "false"
  enable-remote-node-identity: "true"
  operator-api-serve-addr: "127.0.0.1:9234"
  # Enable Hubble gRPC service.
  enable-hubble: "true"
  # UNIX domain socket for Hubble server to listen to.
  hubble-socket-path:  "/var/run/cilium/hubble.sock"
  # An additional address for Hubble server to listen to (e.g. ":4244").
  hubble-listen-address: ":4244"
  hubble-disable-tls: "false"
  hubble-tls-cert-file: /var/lib/cilium/tls/hubble/server.crt
  hubble-tls-key-file: /var/lib/cilium/tls/hubble/server.key
  hubble-tls-client-ca-files: /var/lib/cilium/tls/hubble/client-ca.crt
  ipam: "cluster-pool"
  cluster-pool-ipv4-cidr: "10.244.0.0/16"
  cluster-pool-ipv4-mask-size: "24"
  disable-cnp-status-updates: "true"
  cgroup-root: "/run/cilium/cgroupv2"
---
# Source: cilium/templates/cilium-agent-clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cilium
rules:
- apiGroups:
  - networking.k8s.io
  resources:
  - networkpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  - services
  - nodes
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  - pods/finalizers
  verbs:
  - get
  - list
  - watch
  - update
  - delete
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
  - update
- apiGroups:
  - ""
  resources:
  - nodes
  - nodes/status
  verbs:
  - patch
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  # Deprecated for removal in v1.10
  - create
  - list
  - watch
  - update

  # This is used when validating policies in preflight. This will need to stay
  # until we figure out how to avoid "get" inside the preflight, and then
  # should be removed ideally.
  - get
- apiGroups:
  - cilium.io
  resources:
  - ciliumnetworkpolicies
  - ciliumnetworkpolicies/status
  - ciliumnetworkpolicies/finalizers
  - ciliumclusterwidenetworkpolicies
  - ciliumclusterwidenetworkpolicies/status
  - ciliumclusterwidenetworkpolicies/finalizers
  - ciliumendpoints
  - ciliumendpoints/status
  - ciliumendpoints/finalizers
  - ciliumnodes
  - ciliumnodes/status
  - ciliumnodes/finalizers
  - ciliumidentities
  - ciliumidentities/finalizers
  - ciliumlocalredirectpolicies
  - ciliumlocalredirectpolicies/status
  - ciliumlocalredirectpolicies/finalizers
  verbs:
  - '*'
---
# Source: cilium/templates/cilium-operator-clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cilium-operator
rules:
- apiGroups:
  - ""
  resources:
  # to automatically delete [core|kube]dns pods so that are starting to being
  # managed by Cilium
  - pods
  verbs:
  - get
  - list
  - watch
  - delete
  - apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  # To remove node taints
  - nodes
  # To set NetworkUnavailable false on startup
  - nodes/status
  verbs:
  - patch
- apiGroups:
  - discovery.k8s.io
  resources:
  - endpointslices
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  # to perform the translation of a CNP that contains 'ToGroup' to its endpoints
  - services
  - endpoints
  # to check apiserver connectivity
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - cilium.io
  resources:
  - ciliumnetworkpolicies
  - ciliumnetworkpolicies/status
  - ciliumnetworkpolicies/finalizers
  - ciliumclusterwidenetworkpolicies
  - ciliumclusterwidenetworkpolicies/status
  - ciliumclusterwidenetworkpolicies/finalizers
  - ciliumendpoints
  - ciliumendpoints/status
  - ciliumendpoints/finalizers
  - ciliumnodes
  - ciliumnodes/status
  - ciliumnodes/finalizers
  - ciliumidentities
  - ciliumidentities/status
  - ciliumidentities/finalizers
  - ciliumlocalredirectpolicies
  - ciliumlocalredirectpolicies/status
  - ciliumlocalredirectpolicies/finalizers
  verbs:
  - '*'
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - create
  - get
  - list
  - update
  - watch
# For cilium-operator running in HA mode.
#
# Cilium operator running in HA mode requires the use of ResourceLock for Leader Election
# between multiple running instances.
# The preferred way of doing this is to use LeasesResourceLock as edits to Leases are less
# common and fewer objects in the cluster watch "all Leases".
# The support for leases was introduced in coordination.k8s.io/v1 during Kubernetes 1.14 release.
# In Cilium we currently don't support HA mode for K8s version < 1.14. This condition make sure
# that we only authorize access to leases resources in supported K8s versions.
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - create
  - get
  - update
---
# Source: cilium/templates/cilium-agent-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cilium
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium
subjects:
- kind: ServiceAccount
  name: cilium
  namespace: kube-system
---
# Source: cilium/templates/cilium-operator-clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cilium-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cilium-operator
subjects:
- kind: ServiceAccount
  name: cilium-operator
  namespace: kube-system
---
# Source: cilium/templates/cilium-agent-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    k8s-app: cilium
  name: cilium
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: cilium
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 2
    type: RollingUpdate
  template:
    metadata:
      annotations:
        # This annotation plus the CriticalAddonsOnly toleration makes
        # cilium to be a critical pod in the cluster, which ensures cilium
        # gets priority scheduling.
        # https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        k8s-app: cilium
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - cilium
            topologyKey: kubernetes.io/hostname
      containers:
      - args:
        - --config-dir=/tmp/cilium/config-map
        command:
        - cilium-agent
        livenessProbe:
          httpGet:
            host: '127.0.0.1'
            path: /healthz
            port: 9879
            scheme: HTTP
            httpHeaders:
            - name: "brief"
              value: "true"
          failureThreshold: 10
          # The initial delay for the liveness probe is intentionally large to
          # avoid an endless kill & restart cycle if in the event that the initial
          # bootstrapping takes longer than expected.
          initialDelaySeconds: 120
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            host: '127.0.0.1'
            path: /healthz
            port: 9879
            scheme: HTTP
            httpHeaders:
            - name: "brief"
              value: "true"
          failureThreshold: 3
          initialDelaySeconds: 5
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        env:
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: CILIUM_K8S_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: CILIUM_FLANNEL_MASTER_DEVICE
          valueFrom:
            configMapKeyRef:
              key: flannel-master-device
              name: cilium-config
              optional: true
        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT
          valueFrom:
            configMapKeyRef:
              key: flannel-uninstall-on-exit
              name: cilium-config
              optional: true
        - name: CILIUM_CLUSTERMESH_CONFIG
          value: /var/lib/cilium/clustermesh/
        - name: CILIUM_CNI_CHAINING_MODE
          valueFrom:
            configMapKeyRef:
              key: cni-chaining-mode
              name: cilium-config
              optional: true
        - name: CILIUM_CUSTOM_CNI_CONF
          valueFrom:
            configMapKeyRef:
              key: custom-cni-conf
              name: cilium-config
              optional: true
        image: "quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826"
        imagePullPolicy: IfNotPresent
        lifecycle:
          postStart:
            exec:
              command:
              - "/cni-install.sh"
              - "--enable-debug=false"
          preStop:
            exec:
              command:
              - /cni-uninstall.sh
        name: cilium-agent
        securityContext:
          capabilities:
            add:
            - NET_ADMIN
            - SYS_MODULE
          privileged: true
        volumeMounts:
        - mountPath: /sys/fs/bpf
          name: bpf-maps
        - mountPath: /var/run/cilium
          name: cilium-run
        - mountPath: /host/opt/cni/bin
          name: cni-path
        - mountPath: /host/etc/cni/net.d
          name: etc-cni-netd
        - mountPath: /var/lib/cilium/clustermesh
          name: clustermesh-secrets
          readOnly: true
        - mountPath: /tmp/cilium/config-map
          name: cilium-config-path
          readOnly: true
          # Needed to be able to load kernel modules
        - mountPath: /lib/modules
          name: lib-modules
          readOnly: true
        - mountPath: /run/xtables.lock
          name: xtables-lock
        - mountPath: /var/lib/cilium/tls/hubble
          name: hubble-tls
          readOnly: true
      hostNetwork: true
      initContainers:
      # Required to mount cgroup2 filesystem on the underlying Kubernetes node.
      # We use nsenter command with host's cgroup and mount namespaces enabled.
      - name: mount-cgroup
        env:
          - name: CGROUP_ROOT
            value: /run/cilium/cgroupv2
          - name: BIN_PATH
            value: /opt/cni/bin
        command:
          - sh
          - -c
          # The statically linked Go program binary is invoked to avoid any
          # dependency on utilities like sh and mount that can be missing on certain
          # distros installed on the underlying host. Copy the binary to the
          # same directory where we install cilium cni plugin so that exec permissions
          # are available.
          - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount && nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt "${BIN_PATH}/cilium-mount" $CGROUP_ROOT; rm /hostbin/cilium-mount'
        image: "quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826"
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - mountPath: /hostproc
            name: hostproc
          - mountPath: /hostbin
            name: cni-path
        securityContext:
          privileged: true
      - command:
        - /init-container.sh
        env:
        - name: CILIUM_ALL_STATE
          valueFrom:
            configMapKeyRef:
              key: clean-cilium-state
              name: cilium-config
              optional: true
        - name: CILIUM_BPF_STATE
          valueFrom:
            configMapKeyRef:
              key: clean-cilium-bpf-state
              name: cilium-config
              optional: true
        - name: CILIUM_WAIT_BPF_MOUNT
          valueFrom:
            configMapKeyRef:
              key: wait-bpf-mount
              name: cilium-config
              optional: true
        image: "quay.io/cilium/cilium:v1.12.3@sha256:30de50c4dc0a1e1077e9e7917a54d5cab253058b3f779822aec00f5c817ca826"
        imagePullPolicy: IfNotPresent
        name: clean-cilium-state
        securityContext:
          capabilities:
            add:
            - NET_ADMIN
          privileged: true
        volumeMounts:
        - mountPath: /sys/fs/bpf
          name: bpf-maps
          mountPropagation: HostToContainer
          # Required to mount cgroup filesystem from the host to cilium agent pod
        - mountPath: /run/cilium/cgroupv2
          name: cilium-cgroup
          mountPropagation: HostToContainer
        - mountPath: /var/run/cilium
          name: cilium-run
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
      restartPolicy: Always
      priorityClassName: system-node-critical
      serviceAccount: cilium
      serviceAccountName: cilium
      terminationGracePeriodSeconds: 1
      tolerations:
      - operator: Exists
      volumes:
        # To keep state between restarts / upgrades
      - hostPath:
          path: /var/run/cilium
          type: DirectoryOrCreate
        name: cilium-run
        # To keep state between restarts / upgrades for bpf maps
      - hostPath:
          path: /sys/fs/bpf
          type: DirectoryOrCreate
        name: bpf-maps
      # To mount cgroup2 filesystem on the host
      - hostPath:
          path: /proc
          type: Directory
        name: hostproc
      # To keep state between restarts / upgrades for cgroup2 filesystem
      - hostPath:
          path: /run/cilium/cgroupv2
          type: DirectoryOrCreate
        name: cilium-cgroup
      # To install cilium cni plugin in the host
      - hostPath:
          path:  /opt/cni/bin
          type: DirectoryOrCreate
        name: cni-path
        # To install cilium cni configuration in the host
      - hostPath:
          path: /etc/cni/net.d
          type: DirectoryOrCreate
        name: etc-cni-netd
        # To be able to load kernel modules
      - hostPath:
          path: /lib/modules
        name: lib-modules
        # To access iptables concurrently with other processes (e.g. kube-proxy)
      - hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
        name: xtables-lock
        # To read the clustermesh configuration
      - name: clustermesh-secrets
        secret:
          defaultMode: 420
          optional: true
          secretName: cilium-clustermesh
        # To read the configuration from the config map
      - configMap:
          name: cilium-config
        name: cilium-config-path
      - name: hubble-tls
        projected:
          sources:
          - secret:
              name: hubble-server-certs
              items:
                - key: tls.crt
                  path: server.crt
                - key: tls.key
                  path: server.key
              optional: true
          - configMap:
              name: hubble-ca-cert
              items:
                - key: ca.crt
                  path: client-ca.crt
              optional: true
---
# Source: cilium/templates/cilium-operator-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    io.cilium/app: operator
    name: cilium-operator
  name: cilium-operator
  namespace: kube-system
spec:
  # We support HA mode only for Kubernetes version > 1.14
  # See docs on ServerCapabilities.LeasesResourceLock in file pkg/k8s/version/version.go
  # for more details.
  replicas: 1
  selector:
    matchLabels:
      io.cilium/app: operator
      name: cilium-operator
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
      labels:
        io.cilium/app: operator
        name: cilium-operator
    spec:
      # In HA mode, cilium-operator pods must not be scheduled on the same
      # node as they will clash with each other.
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: io.cilium/app
                operator: In
                values:
                - operator
            topologyKey: kubernetes.io/hostname
      containers:
      - args:
        - --config-dir=/tmp/cilium/config-map
        - --debug=$(CILIUM_DEBUG)
        command:
        - cilium-operator-generic
        env:
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        - name: CILIUM_K8S_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: CILIUM_DEBUG
          valueFrom:
            configMapKeyRef:
              key: debug
              name: cilium-config
              optional: true
        image: "quay.io/cilium/operator-generic:v1.12.3@sha256:816ec1da586139b595eeb31932c61a7c13b07fb4a0255341c0e0f18608e84eff"
        imagePullPolicy: IfNotPresent
        name: cilium-operator
        livenessProbe:
          httpGet:
            host: '127.0.0.1'
            path: /healthz
            port: 9234
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 3
        volumeMounts:
        - mountPath: /tmp/cilium/config-map
          name: cilium-config-path
          readOnly: true
      hostNetwork: true
      restartPolicy: Always
      priorityClassName: system-cluster-critical
      serviceAccount: cilium-operator
      serviceAccountName: cilium-operator
      tolerations:
      - operator: Exists
      volumes:
        # To read the configuration from the config map
      - configMap:
          name: cilium-config
        name: cilium-config-path

I0128 15:42:39.105575  492188 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.4/kubectl ...
I0128 15:42:39.105580  492188 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (23434 bytes)
I0128 15:42:39.111188  492188 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.4/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0128 15:42:39.515334  492188 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0128 15:42:39.515402  492188 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.4/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0128 15:42:39.515440  492188 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.4/kubectl label nodes minikube.k8s.io/version=v1.32.0 minikube.k8s.io/commit=1424a92be68ed1b3b4f42169c457b977a371c3dd minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2024_01_28T15_42_39_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I0128 15:42:39.519824  492188 ops.go:34] apiserver oom_adj: 0
I0128 15:42:39.519827  492188 ops.go:39] adjusting apiserver oom_adj to -10
I0128 15:42:39.519831  492188 ssh_runner.go:195] Run: /bin/bash -c "echo -10 | sudo tee /proc/$(pgrep kube-apiserver)/oom_adj"
I0128 15:42:39.575785  492188 kubeadm.go:1088] duration metric: took 60.446907ms to wait for elevateKubeSystemPrivileges.
I0128 15:42:39.575827  492188 kubeadm.go:406] StartCluster complete in 6.434079507s
I0128 15:42:39.575838  492188 settings.go:142] acquiring lock: {Name:mke1f7b27d1ac840b052db7935458c5facaf161f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:42:39.575876  492188 settings.go:150] Updating kubeconfig:  /home/ernado/.kube/config
I0128 15:42:39.576763  492188 lock.go:35] WriteFile acquiring /home/ernado/.kube/config: {Name:mk33c411ca3cd526930acce573d319a0b251a907 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 15:42:39.576888  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0128 15:42:39.576967  492188 addons.go:505] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0128 15:42:39.576992  492188 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0128 15:42:39.576995  492188 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0128 15:42:39.577000  492188 addons.go:237] Setting addon storage-provisioner=true in "minikube"
I0128 15:42:39.577001  492188 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0128 15:42:39.577022  492188 host.go:66] Checking if "minikube" exists ...
I0128 15:42:39.577038  492188 config.go:182] Loaded profile config "minikube": Driver=kvm2, ContainerRuntime=porto, KubernetesVersion=v1.28.4
I0128 15:42:39.577189  492188 main.go:141] libmachine: Found binary path at /home/ernado/.minikube/bin/docker-machine-driver-kvm2
I0128 15:42:39.577189  492188 main.go:141] libmachine: Found binary path at /home/ernado/.minikube/bin/docker-machine-driver-kvm2
I0128 15:42:39.577205  492188 main.go:141] libmachine: Launching plugin server for driver kvm2
I0128 15:42:39.577296  492188 main.go:141] libmachine: Launching plugin server for driver kvm2
I0128 15:42:39.584814  492188 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:34275
I0128 15:42:39.584816  492188 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:36303
I0128 15:42:39.584996  492188 main.go:141] libmachine: () Calling .GetVersion
I0128 15:42:39.585023  492188 main.go:141] libmachine: () Calling .GetVersion
I0128 15:42:39.585190  492188 main.go:141] libmachine: Using API Version  1
I0128 15:42:39.585194  492188 main.go:141] libmachine: () Calling .SetConfigRaw
I0128 15:42:39.585234  492188 main.go:141] libmachine: Using API Version  1
I0128 15:42:39.585239  492188 main.go:141] libmachine: () Calling .SetConfigRaw
I0128 15:42:39.585306  492188 main.go:141] libmachine: () Calling .GetMachineName
I0128 15:42:39.585350  492188 main.go:141] libmachine: () Calling .GetMachineName
I0128 15:42:39.585403  492188 main.go:141] libmachine: (minikube) Calling .GetState
I0128 15:42:39.585486  492188 main.go:141] libmachine: Found binary path at /home/ernado/.minikube/bin/docker-machine-driver-kvm2
I0128 15:42:39.585499  492188 main.go:141] libmachine: Launching plugin server for driver kvm2
I0128 15:42:39.586746  492188 addons.go:237] Setting addon default-storageclass=true in "minikube"
I0128 15:42:39.586759  492188 host.go:66] Checking if "minikube" exists ...
I0128 15:42:39.586866  492188 main.go:141] libmachine: Found binary path at /home/ernado/.minikube/bin/docker-machine-driver-kvm2
I0128 15:42:39.586877  492188 main.go:141] libmachine: Launching plugin server for driver kvm2
I0128 15:42:39.593131  492188 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:37987
I0128 15:42:39.593232  492188 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:33973
I0128 15:42:39.593313  492188 main.go:141] libmachine: () Calling .GetVersion
I0128 15:42:39.593371  492188 main.go:141] libmachine: () Calling .GetVersion
I0128 15:42:39.593492  492188 main.go:141] libmachine: Using API Version  1
I0128 15:42:39.593495  492188 main.go:141] libmachine: () Calling .SetConfigRaw
I0128 15:42:39.593537  492188 main.go:141] libmachine: Using API Version  1
I0128 15:42:39.593541  492188 main.go:141] libmachine: () Calling .SetConfigRaw
I0128 15:42:39.593595  492188 main.go:141] libmachine: () Calling .GetMachineName
I0128 15:42:39.593642  492188 main.go:141] libmachine: (minikube) Calling .GetState
I0128 15:42:39.593655  492188 main.go:141] libmachine: () Calling .GetMachineName
I0128 15:42:39.593817  492188 main.go:141] libmachine: Found binary path at /home/ernado/.minikube/bin/docker-machine-driver-kvm2
I0128 15:42:39.593828  492188 main.go:141] libmachine: Launching plugin server for driver kvm2
I0128 15:42:39.594352  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:42:39.595462  492188 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0128 15:42:39.596465  492188 addons.go:429] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0128 15:42:39.596470  492188 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0128 15:42:39.596478  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:39.597706  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:39.597858  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:39.597867  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:39.597915  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:39.597970  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:39.598023  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:39.598065  492188 sshutil.go:53] new ssh client: &{IP:192.168.39.62 Port:22 SSHKeyPath:/home/ernado/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:42:39.599791  492188 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:32871
I0128 15:42:39.599955  492188 main.go:141] libmachine: () Calling .GetVersion
I0128 15:42:39.600119  492188 main.go:141] libmachine: Using API Version  1
I0128 15:42:39.600126  492188 main.go:141] libmachine: () Calling .SetConfigRaw
I0128 15:42:39.600245  492188 main.go:141] libmachine: () Calling .GetMachineName
I0128 15:42:39.600298  492188 main.go:141] libmachine: (minikube) Calling .GetState
I0128 15:42:39.600859  492188 main.go:141] libmachine: (minikube) Calling .DriverName
I0128 15:42:39.600932  492188 addons.go:429] installing /etc/kubernetes/addons/storageclass.yaml
I0128 15:42:39.600936  492188 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0128 15:42:39.600940  492188 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0128 15:42:39.602024  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:39.602159  492188 main.go:141] libmachine: (minikube) DBG | found host DHCP lease matching {name: "", mac: "52:54:00:96:58:99", ip: ""} in network mk-minikube: {Iface:virbr1 ExpiryTime:2024-01-28 16:41:58 +0300 MSK Type:0 Mac:52:54:00:96:58:99 Iaid: IPaddr:192.168.39.62 Prefix:24 Hostname:minikube Clientid:01:52:54:00:96:58:99}
I0128 15:42:39.602167  492188 main.go:141] libmachine: (minikube) DBG | domain minikube has defined IP address 192.168.39.62 and MAC address 52:54:00:96:58:99 in network mk-minikube
I0128 15:42:39.602209  492188 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0128 15:42:39.602259  492188 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0128 15:42:39.602306  492188 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0128 15:42:39.602342  492188 sshutil.go:53] new ssh client: &{IP:192.168.39.62 Port:22 SSHKeyPath:/home/ernado/.minikube/machines/minikube/id_rsa Username:docker}
I0128 15:42:39.614351  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.39.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0128 15:42:39.685148  492188 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0128 15:42:39.691251  492188 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0128 15:42:39.731746  492188 start.go:929] {"host.minikube.internal": 192.168.39.1} host record injected into CoreDNS's ConfigMap
I0128 15:42:39.897553  492188 main.go:141] libmachine: Making call to close driver server
I0128 15:42:39.897564  492188 main.go:141] libmachine: (minikube) Calling .Close
I0128 15:42:39.897555  492188 main.go:141] libmachine: Making call to close driver server
I0128 15:42:39.897607  492188 main.go:141] libmachine: (minikube) Calling .Close
I0128 15:42:39.897742  492188 main.go:141] libmachine: Successfully made call to close driver server
I0128 15:42:39.897749  492188 main.go:141] libmachine: Making call to close connection to plugin binary
I0128 15:42:39.897755  492188 main.go:141] libmachine: Making call to close driver server
I0128 15:42:39.897756  492188 main.go:141] libmachine: Successfully made call to close driver server
I0128 15:42:39.897759  492188 main.go:141] libmachine: (minikube) Calling .Close
I0128 15:42:39.897759  492188 main.go:141] libmachine: Making call to close connection to plugin binary
I0128 15:42:39.897763  492188 main.go:141] libmachine: Making call to close driver server
I0128 15:42:39.897765  492188 main.go:141] libmachine: (minikube) Calling .Close
I0128 15:42:39.897929  492188 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0128 15:42:39.897946  492188 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0128 15:42:39.897950  492188 main.go:141] libmachine: Successfully made call to close driver server
I0128 15:42:39.897952  492188 main.go:141] libmachine: Making call to close connection to plugin binary
I0128 15:42:39.897963  492188 main.go:141] libmachine: Successfully made call to close driver server
I0128 15:42:39.897968  492188 main.go:141] libmachine: Making call to close connection to plugin binary
I0128 15:42:39.900030  492188 main.go:141] libmachine: Making call to close driver server
I0128 15:42:39.900033  492188 main.go:141] libmachine: (minikube) Calling .Close
I0128 15:42:39.900117  492188 main.go:141] libmachine: Successfully made call to close driver server
I0128 15:42:39.900122  492188 main.go:141] libmachine: Making call to close connection to plugin binary
I0128 15:42:39.900122  492188 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0128 15:42:39.901280  492188 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0128 15:42:39.902237  492188 addons.go:508] enable addons completed in 325.271874ms: enabled=[storage-provisioner default-storageclass]
I0128 15:42:40.079546  492188 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0128 15:42:40.079560  492188 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.39.62 Port:8443 KubernetesVersion:v1.28.4 ContainerRuntime:porto ControlPlane:true Worker:true}
I0128 15:42:40.080541  492188 out.go:177] üîé  Verifying Kubernetes components...
I0128 15:42:40.081557  492188 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0128 15:42:40.086945  492188 api_server.go:52] waiting for apiserver process to appear ...
I0128 15:42:40.086970  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:40.587858  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:41.087735  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:41.587080  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:42.087591  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:42.587140  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:43.087728  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:43.587100  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:44.087617  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:44.588054  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:45.087497  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:45.587776  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:46.087077  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:46.587371  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:47.087804  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:47.587359  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:48.087753  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:48.587919  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:49.088006  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:49.587331  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:50.087600  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:50.587989  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:51.087369  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:51.587488  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:52.087864  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:52.587494  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:53.088100  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:53.587771  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:54.087807  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:54.587134  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:55.087500  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:55.587406  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:56.087418  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:56.587760  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:57.087787  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:57.587697  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:58.087367  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:58.587871  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:59.087138  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:42:59.587175  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:00.087124  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:00.587929  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:01.087065  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:01.587466  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:02.087450  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:02.587095  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:03.087520  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:03.587622  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:04.087756  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:04.587737  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:05.087155  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:05.587246  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:06.087379  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:06.587629  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:07.087644  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:07.587976  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:08.087557  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:08.588079  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:09.087251  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:09.587833  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:10.087356  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:10.587533  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:11.087973  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:11.587542  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:12.087045  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:12.587826  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:13.087220  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:13.587098  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:14.087424  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:14.587730  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:15.087450  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:15.587638  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:16.087970  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:16.587046  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:17.087974  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:17.587761  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:18.087327  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:18.587683  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:19.087894  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:19.588074  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:20.087931  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:20.587352  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:21.087302  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:21.587696  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:22.087182  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:22.587895  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:23.087207  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:23.587273  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:24.087905  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:24.587396  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:25.087861  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:25.587835  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:26.087703  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:26.587998  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:27.087984  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:27.587598  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:28.087871  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:28.587748  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:29.088053  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:29.587209  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:30.087591  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:30.587139  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:31.087937  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:31.587635  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:32.087290  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:32.587035  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:33.087086  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:33.587972  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:34.087439  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:34.587962  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:35.087970  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:35.587780  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:36.087961  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:36.587761  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:37.087391  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:37.587922  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:38.087143  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:38.587102  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:39.087875  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:39.587645  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:40.087662  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:43:40.087731  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:43:40.103428  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:43:40.103432  492188 cri.go:89] found id: ""
I0128 15:43:40.103437  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:43:40.103494  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:40.104794  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:43:40.104833  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:43:40.118611  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:43:40.118614  492188 cri.go:89] found id: ""
I0128 15:43:40.118617  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:43:40.118649  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:40.119858  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:43:40.119890  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:43:40.133388  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:43:40.133392  492188 cri.go:89] found id: ""
I0128 15:43:40.133394  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:43:40.133434  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:40.134556  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:43:40.134584  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:43:40.159033  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:43:40.159036  492188 cri.go:89] found id: ""
I0128 15:43:40.159038  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:43:40.159070  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:40.160233  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:43:40.160261  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:43:40.173759  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:43:40.173763  492188 cri.go:89] found id: ""
I0128 15:43:40.173767  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:43:40.173807  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:40.174914  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:43:40.174939  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:43:40.190576  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:43:40.190579  492188 cri.go:89] found id: ""
I0128 15:43:40.190580  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:43:40.190608  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:40.191772  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:43:40.191803  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:43:40.205326  492188 cri.go:89] found id: ""
I0128 15:43:40.205330  492188 logs.go:284] 0 containers: []
W0128 15:43:40.205334  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:43:40.205338  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:43:40.205342  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:43:40.250241  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:43:40.250249  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:43:40.266722  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:43:40.266728  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:43:40.279659  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:43:40.279663  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:43:40.295454  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:43:40.295459  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:43:40.308580  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:43:40.308586  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:43:40.327146  492188 logs.go:123] Gathering logs for porto ...
I0128 15:43:40.327150  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:43:40.376664  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:43:40.376673  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:43:40.393277  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:43:40.393359  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:43:40.400284  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:43:40.400290  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:43:40.418898  492188 logs.go:123] Gathering logs for container status ...
I0128 15:43:40.418904  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:43:40.438720  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:43:40.438733  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:43:40.444283  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:43:40.444295  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:43:40.444328  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:43:40.444338  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:43:40.444345  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:43:40.444354  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:43:40.444357  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:43:50.445715  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:43:50.450297  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:43:50.450351  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:43:50.462195  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:43:50.462201  492188 cri.go:89] found id: ""
I0128 15:43:50.462206  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:43:50.462251  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:50.463427  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:43:50.463449  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:43:50.474376  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:43:50.474378  492188 cri.go:89] found id: ""
I0128 15:43:50.474380  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:43:50.474405  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:50.475396  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:43:50.475463  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:43:50.485861  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:43:50.485866  492188 cri.go:89] found id: ""
I0128 15:43:50.485870  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:43:50.485914  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:50.486909  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:43:50.486935  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:43:50.498482  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:43:50.498488  492188 cri.go:89] found id: ""
I0128 15:43:50.498491  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:43:50.498532  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:50.499593  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:43:50.499619  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:43:50.512290  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:43:50.512294  492188 cri.go:89] found id: ""
I0128 15:43:50.512297  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:43:50.512368  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:50.513394  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:43:50.513423  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:43:50.523676  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:43:50.523680  492188 cri.go:89] found id: ""
I0128 15:43:50.523682  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:43:50.523721  492188 ssh_runner.go:195] Run: which crictl
I0128 15:43:50.524615  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:43:50.524638  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:43:50.535931  492188 cri.go:89] found id: ""
I0128 15:43:50.535935  492188 logs.go:284] 0 containers: []
W0128 15:43:50.535939  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:43:50.535944  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:43:50.535948  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:43:50.550086  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:43:50.550155  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:43:50.556830  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:43:50.556834  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:43:50.567386  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:43:50.567396  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:43:50.583663  492188 logs.go:123] Gathering logs for porto ...
I0128 15:43:50.583670  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:43:50.630883  492188 logs.go:123] Gathering logs for container status ...
I0128 15:43:50.630890  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:43:50.644716  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:43:50.644722  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:43:50.648368  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:43:50.648372  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:43:50.684892  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:43:50.684900  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:43:50.699438  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:43:50.699447  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:43:50.711805  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:43:50.711809  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:43:50.724505  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:43:50.724508  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:43:50.739671  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:43:50.739681  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:43:50.739713  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:43:50.739724  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:43:50.739733  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:43:50.739739  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:43:50.739742  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:44:00.741207  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:44:00.745993  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:44:00.746035  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:44:00.757974  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:00.757980  492188 cri.go:89] found id: ""
I0128 15:44:00.757984  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:44:00.758031  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:00.759191  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:44:00.759235  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:44:00.770747  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:44:00.770752  492188 cri.go:89] found id: ""
I0128 15:44:00.770756  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:44:00.770797  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:00.771829  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:44:00.771857  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:44:00.783165  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:00.783167  492188 cri.go:89] found id: ""
I0128 15:44:00.783169  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:44:00.783190  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:00.784172  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:44:00.784222  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:44:00.794571  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:00.794577  492188 cri.go:89] found id: ""
I0128 15:44:00.794581  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:44:00.794621  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:00.795551  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:44:00.795575  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:44:00.805910  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:00.805915  492188 cri.go:89] found id: ""
I0128 15:44:00.805918  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:44:00.805953  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:00.807017  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:44:00.807039  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:44:00.818742  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:00.818746  492188 cri.go:89] found id: ""
I0128 15:44:00.818749  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:44:00.818788  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:00.819835  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:44:00.819861  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:44:00.830336  492188 cri.go:89] found id: ""
I0128 15:44:00.830341  492188 logs.go:284] 0 containers: []
W0128 15:44:00.830344  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:44:00.830348  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:44:00.830353  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:00.846804  492188 logs.go:123] Gathering logs for porto ...
I0128 15:44:00.846808  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:44:00.890872  492188 logs.go:123] Gathering logs for container status ...
I0128 15:44:00.890882  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:44:00.905675  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:44:00.905684  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:44:00.920740  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:00.920818  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:00.928287  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:44:00.928290  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:44:00.941134  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:44:00.941140  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:00.955900  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:44:00.955905  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:00.966769  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:44:00.966773  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:00.979152  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:44:00.979158  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:00.992038  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:44:00.992044  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:44:00.995958  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:44:00.995961  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:44:01.034487  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:01.034500  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:44:01.034530  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:44:01.034541  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:01.034547  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:01.034555  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:01.034558  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:44:11.035439  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:44:11.040419  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:44:11.040457  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:44:11.055930  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:11.055934  492188 cri.go:89] found id: ""
I0128 15:44:11.055938  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:44:11.055969  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:11.057254  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:44:11.057298  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:44:11.071523  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:44:11.071528  492188 cri.go:89] found id: ""
I0128 15:44:11.071533  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:44:11.071570  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:11.072620  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:44:11.072646  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:44:11.087032  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:11.087038  492188 cri.go:89] found id: ""
I0128 15:44:11.087042  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:44:11.087077  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:11.088167  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:44:11.088197  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:44:11.102659  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:11.102665  492188 cri.go:89] found id: ""
I0128 15:44:11.102669  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:44:11.102709  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:11.103827  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:44:11.103867  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:44:11.115284  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:11.115289  492188 cri.go:89] found id: ""
I0128 15:44:11.115293  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:44:11.115337  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:11.116323  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:44:11.116347  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:44:11.128112  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:11.128117  492188 cri.go:89] found id: ""
I0128 15:44:11.128122  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:44:11.128155  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:11.129206  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:44:11.129243  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:44:11.142003  492188 cri.go:89] found id: ""
I0128 15:44:11.142010  492188 logs.go:284] 0 containers: []
W0128 15:44:11.142014  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:44:11.142019  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:44:11.142026  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:11.159039  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:44:11.159047  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:11.173563  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:44:11.173571  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:11.190208  492188 logs.go:123] Gathering logs for porto ...
I0128 15:44:11.190221  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:44:11.240610  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:44:11.240619  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:44:11.255093  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:11.255168  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:11.263394  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:44:11.263399  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:44:11.309920  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:44:11.309924  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:44:11.323932  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:44:11.323935  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:11.334380  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:44:11.334383  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:11.344965  492188 logs.go:123] Gathering logs for container status ...
I0128 15:44:11.344969  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:44:11.357852  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:44:11.357856  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:44:11.361804  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:11.361815  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:44:11.361838  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:44:11.361848  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:11.361854  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:11.361859  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:11.361862  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:44:21.363388  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:44:21.368346  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:44:21.368399  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:44:21.382896  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:21.382900  492188 cri.go:89] found id: ""
I0128 15:44:21.382903  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:44:21.382933  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:21.383966  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:44:21.383992  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:44:21.394736  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:44:21.394744  492188 cri.go:89] found id: ""
I0128 15:44:21.394749  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:44:21.394806  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:21.395791  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:44:21.395819  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:44:21.406426  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:21.406433  492188 cri.go:89] found id: ""
I0128 15:44:21.406437  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:44:21.406486  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:21.407497  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:44:21.407529  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:44:21.425152  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:21.425155  492188 cri.go:89] found id: ""
I0128 15:44:21.425157  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:44:21.425182  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:21.426534  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:44:21.426558  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:44:21.441084  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:21.441089  492188 cri.go:89] found id: ""
I0128 15:44:21.441092  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:44:21.441129  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:21.442254  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:44:21.442306  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:44:21.454874  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:21.454880  492188 cri.go:89] found id: ""
I0128 15:44:21.454883  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:44:21.454923  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:21.455996  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:44:21.456031  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:44:21.467212  492188 cri.go:89] found id: ""
I0128 15:44:21.467217  492188 logs.go:284] 0 containers: []
W0128 15:44:21.467221  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:44:21.467225  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:44:21.467229  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:21.480900  492188 logs.go:123] Gathering logs for porto ...
I0128 15:44:21.480908  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:44:21.531354  492188 logs.go:123] Gathering logs for container status ...
I0128 15:44:21.531360  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:44:21.545348  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:44:21.545353  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:44:21.562188  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:21.562263  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:21.570918  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:44:21.570923  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:44:21.574996  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:44:21.575001  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:21.589162  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:44:21.589166  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:21.600063  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:44:21.600068  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:44:21.640657  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:44:21.640663  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:44:21.657658  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:44:21.657662  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:21.668931  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:44:21.668937  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:21.685263  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:21.685277  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:44:21.685303  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:44:21.685313  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:21.685318  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:21.685323  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:21.685326  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:44:31.687048  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:44:31.692041  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:44:31.692082  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:44:31.705789  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:31.705799  492188 cri.go:89] found id: ""
I0128 15:44:31.705804  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:44:31.705846  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:31.706949  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:44:31.706980  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:44:31.719067  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:44:31.719075  492188 cri.go:89] found id: ""
I0128 15:44:31.719080  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:44:31.719151  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:31.720221  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:44:31.720249  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:44:31.731491  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:31.731494  492188 cri.go:89] found id: ""
I0128 15:44:31.731497  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:44:31.731523  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:31.732525  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:44:31.732552  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:44:31.743190  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:31.743198  492188 cri.go:89] found id: ""
I0128 15:44:31.743203  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:44:31.743256  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:31.744297  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:44:31.744327  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:44:31.756907  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:31.756912  492188 cri.go:89] found id: ""
I0128 15:44:31.756915  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:44:31.756957  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:31.758006  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:44:31.758035  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:44:31.768977  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:31.768981  492188 cri.go:89] found id: ""
I0128 15:44:31.768982  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:44:31.769011  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:31.770021  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:44:31.770066  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:44:31.781334  492188 cri.go:89] found id: ""
I0128 15:44:31.781342  492188 logs.go:284] 0 containers: []
W0128 15:44:31.781346  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:44:31.781363  492188 logs.go:123] Gathering logs for porto ...
I0128 15:44:31.781368  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:44:31.832830  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:44:31.832837  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:44:31.849503  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:31.849585  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:31.860049  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:44:31.860051  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:44:31.863872  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:44:31.863876  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:44:31.902917  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:44:31.902922  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:31.918449  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:44:31.918454  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:44:31.932193  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:44:31.932206  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:31.942516  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:44:31.942524  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:31.960785  492188 logs.go:123] Gathering logs for container status ...
I0128 15:44:31.960791  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:44:31.975187  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:44:31.975192  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:31.988518  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:44:31.988524  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:31.999492  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:31.999503  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:44:31.999530  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:44:31.999541  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:31.999547  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:31.999553  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:31.999558  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:44:42.000834  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:44:42.005645  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:44:42.005694  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:44:42.017909  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:42.017918  492188 cri.go:89] found id: ""
I0128 15:44:42.017922  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:44:42.017998  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:42.019095  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:44:42.019129  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:44:42.031148  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:44:42.031154  492188 cri.go:89] found id: ""
I0128 15:44:42.031158  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:44:42.031215  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:42.032316  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:44:42.032355  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:44:42.043109  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:42.043119  492188 cri.go:89] found id: ""
I0128 15:44:42.043123  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:44:42.043169  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:42.044220  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:44:42.044251  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:44:42.057739  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:42.057744  492188 cri.go:89] found id: ""
I0128 15:44:42.057747  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:44:42.057801  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:42.058959  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:44:42.059003  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:44:42.071350  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:42.071355  492188 cri.go:89] found id: ""
I0128 15:44:42.071360  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:44:42.071409  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:42.072453  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:44:42.072479  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:44:42.085173  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:42.085176  492188 cri.go:89] found id: ""
I0128 15:44:42.085178  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:44:42.085202  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:42.086174  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:44:42.086205  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:44:42.097207  492188 cri.go:89] found id: ""
I0128 15:44:42.097209  492188 logs.go:284] 0 containers: []
W0128 15:44:42.097212  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:44:42.097216  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:44:42.097220  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:44:42.113874  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:42.113955  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:42.125188  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:44:42.125191  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:42.135760  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:44:42.135766  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:42.148734  492188 logs.go:123] Gathering logs for porto ...
I0128 15:44:42.148738  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:44:42.196005  492188 logs.go:123] Gathering logs for container status ...
I0128 15:44:42.196019  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:44:42.210539  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:44:42.210547  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:44:42.214542  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:44:42.214547  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:44:42.255657  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:44:42.255666  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:42.271680  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:44:42.271692  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:44:42.284542  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:44:42.284546  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:42.295021  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:44:42.295026  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:42.311295  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:42.311304  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:44:42.311330  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:44:42.311340  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:42.311344  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:42.311350  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:42.311352  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:44:52.313050  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:44:52.317746  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:44:52.317775  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:44:52.331286  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:52.331291  492188 cri.go:89] found id: ""
I0128 15:44:52.331295  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:44:52.331339  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:52.332435  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:44:52.332463  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:44:52.343014  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:44:52.343022  492188 cri.go:89] found id: ""
I0128 15:44:52.343027  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:44:52.343084  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:52.344117  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:44:52.344143  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:44:52.356023  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:52.356026  492188 cri.go:89] found id: ""
I0128 15:44:52.356028  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:44:52.356054  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:52.357038  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:44:52.357065  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:44:52.367939  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:52.367942  492188 cri.go:89] found id: ""
I0128 15:44:52.367944  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:44:52.367970  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:52.368948  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:44:52.368974  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:44:52.379506  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:52.379514  492188 cri.go:89] found id: ""
I0128 15:44:52.379519  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:44:52.379580  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:52.380575  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:44:52.380645  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:44:52.392219  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:52.392222  492188 cri.go:89] found id: ""
I0128 15:44:52.392225  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:44:52.392252  492188 ssh_runner.go:195] Run: which crictl
I0128 15:44:52.393299  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:44:52.393336  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:44:52.404013  492188 cri.go:89] found id: ""
I0128 15:44:52.404021  492188 logs.go:284] 0 containers: []
W0128 15:44:52.404024  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:44:52.404030  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:44:52.404037  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:44:52.417944  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:44:52.417950  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:44:52.430343  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:44:52.430354  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:44:52.443440  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:44:52.443444  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:44:52.461291  492188 logs.go:123] Gathering logs for container status ...
I0128 15:44:52.461301  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:44:52.475473  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:44:52.475481  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:44:52.479950  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:44:52.479955  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:44:52.529127  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:44:52.529134  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:44:52.545924  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:44:52.545930  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:44:52.561787  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:52.561860  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:52.572373  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:44:52.572376  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:44:52.585684  492188 logs.go:123] Gathering logs for porto ...
I0128 15:44:52.585694  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:44:52.632657  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:52.632668  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:44:52.632695  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:44:52.632704  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:44:52.632708  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:44:52.632714  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:44:52.632717  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:45:02.633272  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:45:02.638088  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:45:02.638135  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:45:02.650543  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:02.650551  492188 cri.go:89] found id: ""
I0128 15:45:02.650555  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:45:02.650606  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:02.651707  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:45:02.651736  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:45:02.668246  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:45:02.668249  492188 cri.go:89] found id: ""
I0128 15:45:02.668251  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:45:02.668277  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:02.669420  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:45:02.669465  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:45:02.681387  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:02.681392  492188 cri.go:89] found id: ""
I0128 15:45:02.681396  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:45:02.681440  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:02.682502  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:45:02.682533  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:45:02.693892  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:02.693895  492188 cri.go:89] found id: ""
I0128 15:45:02.693897  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:45:02.693923  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:02.694983  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:45:02.695010  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:45:02.707078  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:02.707082  492188 cri.go:89] found id: ""
I0128 15:45:02.707084  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:45:02.707116  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:02.708108  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:45:02.708129  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:45:02.719470  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:02.719479  492188 cri.go:89] found id: ""
I0128 15:45:02.719483  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:45:02.719538  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:02.720570  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:45:02.720601  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:45:02.731144  492188 cri.go:89] found id: ""
I0128 15:45:02.731147  492188 logs.go:284] 0 containers: []
W0128 15:45:02.731150  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:45:02.731155  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:45:02.731159  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:45:02.735139  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:45:02.735146  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:45:02.749961  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:45:02.749965  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:02.764636  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:45:02.764642  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:02.780778  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:45:02.780783  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:02.793659  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:45:02.793663  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:02.812793  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:45:02.812797  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:45:02.829029  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:02.829104  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:02.840154  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:45:02.840159  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:45:02.891020  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:45:02.891030  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:02.907956  492188 logs.go:123] Gathering logs for porto ...
I0128 15:45:02.907965  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:45:02.950354  492188 logs.go:123] Gathering logs for container status ...
I0128 15:45:02.950363  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:45:02.969194  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:02.969206  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:45:02.969233  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:45:02.969243  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:02.969248  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:02.969254  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:02.969257  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:45:12.970108  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:45:12.974928  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:45:12.974968  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:45:12.987044  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:12.987051  492188 cri.go:89] found id: ""
I0128 15:45:12.987056  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:45:12.987113  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:12.988147  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:45:12.988193  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:45:13.000254  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:45:13.000257  492188 cri.go:89] found id: ""
I0128 15:45:13.000261  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:45:13.000297  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:13.001311  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:45:13.001339  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:45:13.012532  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:13.012535  492188 cri.go:89] found id: ""
I0128 15:45:13.012538  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:45:13.012569  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:13.013578  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:45:13.013605  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:45:13.025162  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:13.025166  492188 cri.go:89] found id: ""
I0128 15:45:13.025169  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:45:13.025198  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:13.026205  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:45:13.026236  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:45:13.038573  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:13.038578  492188 cri.go:89] found id: ""
I0128 15:45:13.038582  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:45:13.038624  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:13.039609  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:45:13.039636  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:45:13.051407  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:13.051413  492188 cri.go:89] found id: ""
I0128 15:45:13.051417  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:45:13.051461  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:13.052636  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:45:13.052664  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:45:13.063754  492188 cri.go:89] found id: ""
I0128 15:45:13.063759  492188 logs.go:284] 0 containers: []
W0128 15:45:13.063762  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:45:13.063767  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:45:13.063772  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:13.075029  492188 logs.go:123] Gathering logs for container status ...
I0128 15:45:13.075036  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:45:13.087836  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:45:13.087840  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:13.105221  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:45:13.105225  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:45:13.119800  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:13.119872  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:13.131573  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:45:13.131576  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:45:13.135758  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:45:13.135767  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:45:13.174472  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:45:13.174481  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:13.190439  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:45:13.190447  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:45:13.203743  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:45:13.203749  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:13.214213  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:45:13.214220  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:13.227839  492188 logs.go:123] Gathering logs for porto ...
I0128 15:45:13.227846  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:45:13.270937  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:13.270948  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:45:13.270978  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:45:13.270989  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:13.270995  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:13.271001  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:13.271004  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:45:23.272286  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:45:23.277073  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:45:23.277114  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:45:23.289701  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:23.289706  492188 cri.go:89] found id: ""
I0128 15:45:23.289710  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:45:23.289754  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:23.290899  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:45:23.290925  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:45:23.303791  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:45:23.303797  492188 cri.go:89] found id: ""
I0128 15:45:23.303801  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:45:23.303842  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:23.305011  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:45:23.305048  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:45:23.316572  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:23.316577  492188 cri.go:89] found id: ""
I0128 15:45:23.316581  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:45:23.316623  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:23.317746  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:45:23.317789  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:45:23.330991  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:23.330996  492188 cri.go:89] found id: ""
I0128 15:45:23.330999  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:45:23.331037  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:23.332134  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:45:23.332165  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:45:23.343830  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:23.343834  492188 cri.go:89] found id: ""
I0128 15:45:23.343837  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:45:23.343873  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:23.345018  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:45:23.345059  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:45:23.358136  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:23.358141  492188 cri.go:89] found id: ""
I0128 15:45:23.358144  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:45:23.358181  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:23.359266  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:45:23.359292  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:45:23.370990  492188 cri.go:89] found id: ""
I0128 15:45:23.370995  492188 logs.go:284] 0 containers: []
W0128 15:45:23.370998  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:45:23.371003  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:45:23.371007  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:45:23.386581  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:23.386661  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:23.400592  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:45:23.400595  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:45:23.404508  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:45:23.404512  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:45:23.417928  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:45:23.417938  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:23.429893  492188 logs.go:123] Gathering logs for porto ...
I0128 15:45:23.429898  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:45:23.470983  492188 logs.go:123] Gathering logs for container status ...
I0128 15:45:23.470987  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:45:23.486110  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:45:23.486122  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:45:23.524062  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:45:23.524073  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:23.538799  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:45:23.538806  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:23.549053  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:45:23.549060  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:23.566844  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:45:23.566852  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:23.583164  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:23.583176  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:45:23.583204  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:45:23.583215  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:23.583221  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:23.583227  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:23.583230  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:45:33.583758  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:45:33.588605  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:45:33.588642  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:45:33.606107  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:33.606114  492188 cri.go:89] found id: ""
I0128 15:45:33.606119  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:45:33.606171  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:33.607331  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:45:33.607370  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:45:33.618332  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:45:33.618336  492188 cri.go:89] found id: ""
I0128 15:45:33.618339  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:45:33.618371  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:33.619379  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:45:33.619407  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:45:33.630347  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:33.630354  492188 cri.go:89] found id: ""
I0128 15:45:33.630358  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:45:33.630397  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:33.631402  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:45:33.631437  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:45:33.642913  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:33.642918  492188 cri.go:89] found id: ""
I0128 15:45:33.642921  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:45:33.642961  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:33.644025  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:45:33.644088  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:45:33.655075  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:33.655080  492188 cri.go:89] found id: ""
I0128 15:45:33.655083  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:45:33.655122  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:33.656079  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:45:33.656108  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:45:33.668260  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:33.668265  492188 cri.go:89] found id: ""
I0128 15:45:33.668269  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:45:33.668305  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:33.669285  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:45:33.669311  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:45:33.681301  492188 cri.go:89] found id: ""
I0128 15:45:33.681306  492188 logs.go:284] 0 containers: []
W0128 15:45:33.681310  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:45:33.681314  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:45:33.681320  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:33.694393  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:45:33.694397  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:33.704756  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:45:33.704760  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:33.721642  492188 logs.go:123] Gathering logs for porto ...
I0128 15:45:33.721648  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:45:33.766483  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:45:33.766493  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:33.782048  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:45:33.782054  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:45:33.794344  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:45:33.794353  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:45:33.836312  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:45:33.836325  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:33.848873  492188 logs.go:123] Gathering logs for container status ...
I0128 15:45:33.848882  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:45:33.862682  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:45:33.862690  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:45:33.878219  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:33.878297  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:33.892156  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:45:33.892162  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:45:33.896820  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:33.896838  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:45:33.896873  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:45:33.896888  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:33.896896  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:33.896905  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:33.896909  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:45:43.897357  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:45:43.902222  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:45:43.902269  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:45:43.915289  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:43.915295  492188 cri.go:89] found id: ""
I0128 15:45:43.915299  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:45:43.915335  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:43.916406  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:45:43.916441  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:45:43.927070  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:45:43.927076  492188 cri.go:89] found id: ""
I0128 15:45:43.927080  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:45:43.927128  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:43.928200  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:45:43.928229  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:45:43.938951  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:43.938955  492188 cri.go:89] found id: ""
I0128 15:45:43.938957  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:45:43.938985  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:43.940004  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:45:43.940038  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:45:43.953534  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:43.953539  492188 cri.go:89] found id: ""
I0128 15:45:43.953542  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:45:43.953576  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:43.954598  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:45:43.954626  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:45:43.965136  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:43.965141  492188 cri.go:89] found id: ""
I0128 15:45:43.965144  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:45:43.965184  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:43.966100  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:45:43.966123  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:45:43.977740  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:43.977743  492188 cri.go:89] found id: ""
I0128 15:45:43.977745  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:45:43.977770  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:43.978784  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:45:43.978805  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:45:43.989505  492188 cri.go:89] found id: ""
I0128 15:45:43.989510  492188 logs.go:284] 0 containers: []
W0128 15:45:43.989512  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:45:43.989516  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:45:43.989520  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:45:44.004166  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:44.004240  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:44.018252  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:45:44.018259  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:45:44.022692  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:45:44.022696  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:44.038753  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:45:44.038765  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:45:44.056810  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:45:44.056822  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:44.071194  492188 logs.go:123] Gathering logs for container status ...
I0128 15:45:44.071208  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:45:44.090005  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:45:44.090022  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:45:44.131564  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:45:44.131580  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:44.147047  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:45:44.147057  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:44.159422  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:45:44.159428  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:44.176715  492188 logs.go:123] Gathering logs for porto ...
I0128 15:45:44.176725  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:45:44.218815  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:44.218829  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:45:44.218858  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:45:44.218868  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:44.218872  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:44.218877  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:44.218880  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:45:54.219385  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:45:54.224210  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:45:54.224266  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:45:54.236094  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:54.236101  492188 cri.go:89] found id: ""
I0128 15:45:54.236106  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:45:54.236161  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:54.237112  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:45:54.237136  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:45:54.248419  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:45:54.248425  492188 cri.go:89] found id: ""
I0128 15:45:54.248430  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:45:54.248480  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:54.249480  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:45:54.249511  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:45:54.259968  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:54.259974  492188 cri.go:89] found id: ""
I0128 15:45:54.259978  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:45:54.260022  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:54.260960  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:45:54.260997  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:45:54.271972  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:54.271978  492188 cri.go:89] found id: ""
I0128 15:45:54.271982  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:45:54.272032  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:54.272941  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:45:54.272968  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:45:54.284560  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:54.284565  492188 cri.go:89] found id: ""
I0128 15:45:54.284569  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:45:54.284611  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:54.285531  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:45:54.285553  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:45:54.295947  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:54.295950  492188 cri.go:89] found id: ""
I0128 15:45:54.295953  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:45:54.295978  492188 ssh_runner.go:195] Run: which crictl
I0128 15:45:54.296879  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:45:54.296921  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:45:54.308065  492188 cri.go:89] found id: ""
I0128 15:45:54.308070  492188 logs.go:284] 0 containers: []
W0128 15:45:54.308074  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:45:54.308079  492188 logs.go:123] Gathering logs for container status ...
I0128 15:45:54.308085  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:45:54.319501  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:45:54.319506  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:45:54.333148  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:54.333216  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:54.346352  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:45:54.346355  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:45:54.383874  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:45:54.383883  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:45:54.397554  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:45:54.397561  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:45:54.408984  492188 logs.go:123] Gathering logs for porto ...
I0128 15:45:54.408990  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:45:54.449913  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:45:54.449917  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:45:54.454105  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:45:54.454111  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:45:54.468549  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:45:54.468553  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:45:54.478090  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:45:54.478099  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:45:54.491808  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:45:54.491813  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:45:54.506929  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:54.506941  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:45:54.506972  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:45:54.506985  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:45:54.506992  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:45:54.507000  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:45:54.507003  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:46:04.507757  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:46:04.512630  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:46:04.512672  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:46:04.524750  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:46:04.524754  492188 cri.go:89] found id: ""
I0128 15:46:04.524757  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:46:04.524789  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:04.525888  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:46:04.525917  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:46:04.536720  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:46:04.536726  492188 cri.go:89] found id: ""
I0128 15:46:04.536730  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:46:04.536773  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:04.537764  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:46:04.537789  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:46:04.549426  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:46:04.549429  492188 cri.go:89] found id: ""
I0128 15:46:04.549431  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:46:04.549457  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:04.550392  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:46:04.550445  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:46:04.561016  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:46:04.561018  492188 cri.go:89] found id: ""
I0128 15:46:04.561020  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:46:04.561044  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:04.561967  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:46:04.562003  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:46:04.574026  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:46:04.574032  492188 cri.go:89] found id: ""
I0128 15:46:04.574037  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:46:04.574081  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:04.575106  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:46:04.575138  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:46:04.586123  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:46:04.586129  492188 cri.go:89] found id: ""
I0128 15:46:04.586132  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:46:04.586171  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:04.587137  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:46:04.587165  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:46:04.598019  492188 cri.go:89] found id: ""
I0128 15:46:04.598023  492188 logs.go:284] 0 containers: []
W0128 15:46:04.598026  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:46:04.598030  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:46:04.598034  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:46:04.636132  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:46:04.636143  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:46:04.651730  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:46:04.651735  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:46:04.666112  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:46:04.666117  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:46:04.676776  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:46:04.676782  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:46:04.692432  492188 logs.go:123] Gathering logs for porto ...
I0128 15:46:04.692441  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:46:04.739213  492188 logs.go:123] Gathering logs for container status ...
I0128 15:46:04.739221  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:46:04.755599  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:46:04.755606  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:46:04.772365  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:46:04.772441  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:46:04.787807  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:46:04.787811  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:46:04.791597  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:46:04.791603  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:46:04.805488  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:46:04.805495  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:46:04.816233  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:46:04.816243  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:46:04.816270  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:46:04.816281  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:46:04.816286  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:46:04.816290  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:46:04.816293  492188 out.go:348] isatty.IsTerminal(2) = true
I0128 15:46:14.817081  492188 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 15:46:14.821822  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0128 15:46:14.821852  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0128 15:46:14.834038  492188 cri.go:89] found id: "kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:46:14.834045  492188 cri.go:89] found id: ""
I0128 15:46:14.834050  492188 logs.go:284] 1 containers: [kube-apiserver-minikube-ee59/kube-apiserver-1f90]
I0128 15:46:14.834102  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:14.835120  492188 cri.go:54] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0128 15:46:14.835149  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0128 15:46:14.847738  492188 cri.go:89] found id: "etcd-minikube-ba37/etcd-d578"
I0128 15:46:14.847745  492188 cri.go:89] found id: ""
I0128 15:46:14.847749  492188 logs.go:284] 1 containers: [etcd-minikube-ba37/etcd-d578]
I0128 15:46:14.847803  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:14.848918  492188 cri.go:54] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0128 15:46:14.848947  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0128 15:46:14.860207  492188 cri.go:89] found id: "coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:46:14.860210  492188 cri.go:89] found id: ""
I0128 15:46:14.860214  492188 logs.go:284] 1 containers: [coredns-5dd5756b68-9fmgs-292a/coredns-0dca]
I0128 15:46:14.860241  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:14.861196  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0128 15:46:14.861225  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0128 15:46:14.872138  492188 cri.go:89] found id: "kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:46:14.872141  492188 cri.go:89] found id: ""
I0128 15:46:14.872144  492188 logs.go:284] 1 containers: [kube-scheduler-minikube-463a/kube-scheduler-9b75]
I0128 15:46:14.872175  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:14.873123  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0128 15:46:14.873152  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0128 15:46:14.887217  492188 cri.go:89] found id: "kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:46:14.887221  492188 cri.go:89] found id: ""
I0128 15:46:14.887223  492188 logs.go:284] 1 containers: [kube-proxy-97nfh-8f2d/kube-proxy-321c]
I0128 15:46:14.887254  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:14.888300  492188 cri.go:54] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0128 15:46:14.888331  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0128 15:46:14.900181  492188 cri.go:89] found id: "kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:46:14.900186  492188 cri.go:89] found id: ""
I0128 15:46:14.900189  492188 logs.go:284] 1 containers: [kube-controller-manager-minikube-0889/kube-controller-manager-4349]
I0128 15:46:14.900222  492188 ssh_runner.go:195] Run: which crictl
I0128 15:46:14.901252  492188 cri.go:54] listing CRI containers in root : {State:all Name:kindnet Namespaces:[]}
I0128 15:46:14.901281  492188 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kindnet
I0128 15:46:14.912881  492188 cri.go:89] found id: ""
I0128 15:46:14.912885  492188 logs.go:284] 0 containers: []
W0128 15:46:14.912887  492188 logs.go:286] No container was found matching "kindnet"
I0128 15:46:14.912891  492188 logs.go:123] Gathering logs for coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] ...
I0128 15:46:14.912895  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
I0128 15:46:14.923120  492188 logs.go:123] Gathering logs for kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] ...
I0128 15:46:14.923126  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-proxy-97nfh-8f2d/kube-proxy-321c"
I0128 15:46:14.933526  492188 logs.go:123] Gathering logs for porto ...
I0128 15:46:14.933530  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo tail -n 400 /var/log/portod.log"
I0128 15:46:14.977368  492188 logs.go:123] Gathering logs for kubelet ...
I0128 15:46:14.977376  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0128 15:46:14.996737  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:46:14.996813  492188 logs.go:138] Found kubelet problem: Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:46:15.011929  492188 logs.go:123] Gathering logs for describe nodes ...
I0128 15:46:15.011934  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0128 15:46:15.063742  492188 logs.go:123] Gathering logs for kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] ...
I0128 15:46:15.063756  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-apiserver-minikube-ee59/kube-apiserver-1f90"
I0128 15:46:15.082665  492188 logs.go:123] Gathering logs for etcd [etcd-minikube-ba37/etcd-d578] ...
I0128 15:46:15.082670  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 etcd-minikube-ba37/etcd-d578"
I0128 15:46:15.097513  492188 logs.go:123] Gathering logs for dmesg ...
I0128 15:46:15.097518  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0128 15:46:15.102317  492188 logs.go:123] Gathering logs for kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] ...
I0128 15:46:15.102331  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-scheduler-minikube-463a/kube-scheduler-9b75"
I0128 15:46:15.117948  492188 logs.go:123] Gathering logs for kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] ...
I0128 15:46:15.117952  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo /usr/bin/crictl logs --tail 400 kube-controller-manager-minikube-0889/kube-controller-manager-4349"
I0128 15:46:15.138129  492188 logs.go:123] Gathering logs for container status ...
I0128 15:46:15.138138  492188 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0128 15:46:15.154234  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:46:15.154247  492188 out.go:348] isatty.IsTerminal(2) = true
W0128 15:46:15.154273  492188 out.go:239] ‚ùå  Problems detected in kubelet:
W0128 15:46:15.154283  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: W0128 12:42:51.358418    1524 reflector.go:535] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0128 15:46:15.154288  492188 out.go:239]     Jan 28 12:42:51 minikube kubelet[1524]: E0128 12:42:51.358443    1524 reflector.go:147] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "kube-root-ca.crt" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0128 15:46:15.154293  492188 out.go:309] Setting ErrFile to fd 2...
I0128 15:46:15.154296  492188 out.go:348] isatty.IsTerminal(2) = true


==> container status <==
CONTAINER           IMAGE                                                              CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
cilium-zxb49-       6b32938e13a429dad7437dd60e8d28959a0d6dd6f4d13912f5d302f20af2693f   About a minute ago   Exited              cilium-agent              4                   cilium-zxb49-       cilium-zxb49
cilium-zxb49-       6b32938e13a429dad7437dd60e8d28959a0d6dd6f4d13912f5d302f20af2693f   3 minutes ago        Exited              clean-cilium-state        0                   cilium-zxb49-       cilium-zxb49
cilium-operat       3c8c45f02d055945a3055d2685df5ec72627d6e29b2d6552465fba125f178bca   3 minutes ago        Running             cilium-operator           0                   cilium-operat       cilium-operator-7d98fd79fc-t9sl9
cilium-zxb49-       6b32938e13a429dad7437dd60e8d28959a0d6dd6f4d13912f5d302f20af2693f   3 minutes ago        Exited              mount-cgroup              0                   cilium-zxb49-       cilium-zxb49
coredns-5dd57       ead0a4a53df89fd173874b46093b6e62d8c72967bbf606d672c9e8c9b601a4fc   3 minutes ago        Running             coredns                   0                   coredns-5dd57       coredns-5dd5756b68-9fmgs
storage-provi       6e38f40d628db3002f5617342c8872c935de530d867d0f709a2fbda1a302a562   3 minutes ago        Running             storage-provisioner       0                   storage-provi       storage-provisioner
kube-proxy-97       83f6cc407eed88d214aad97f3539bde5c8e485ff14424cd021a3a2899304398e   3 minutes ago        Running             kube-proxy                0                   kube-proxy-97       kube-proxy-97nfh
etcd-minikube       73deb9a3f702532592a4167455f8bf2e5f5d900bcc959ba2fd2d35c321de1af9   3 minutes ago        Running             etcd                      0                   etcd-minikube       etcd-minikube
kube-apiserve       7fe0e6f37db33464725e616a12ccc4e36970370005a2b09683a974db6350c257   3 minutes ago        Running             kube-apiserver            0                   kube-apiserve       kube-apiserver-minikube
kube-controll       d058aa5ab969ce7b84d25e7188be1f80633b18db8ea7d02d9d0a78e676236591   3 minutes ago        Running             kube-controller-manager   0                   kube-controll       kube-controller-manager-minikube
kube-schedule       e3db313c6dbc065d4ac3b32c7a6f2a878949031b881d217b63881a109c5cfba1   3 minutes ago        Running             kube-scheduler            0                   kube-schedule       kube-scheduler-minikube


==> coredns [coredns-5dd5756b68-9fmgs-292a/coredns-0dca] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 6c8bd46af3d98e03c4ae8e438c65dd0c69a5f817565481bcf1725dd66ff794963b7938c81e3a23d4c2ad9e52f818076e819219c79e8007dd90564767ed68ba4c
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:46802 - 54098 "HINFO IN 3929707794225744340.3507428323326190543. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.058732258s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=1424a92be68ed1b3b4f42169c457b977a371c3dd
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_01_28T15_42_39_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/portoshim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 28 Jan 2024 12:42:36 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 28 Jan 2024 12:46:14 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 28 Jan 2024 12:46:13 +0000   Sun, 28 Jan 2024 12:42:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 28 Jan 2024 12:46:13 +0000   Sun, 28 Jan 2024 12:42:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 28 Jan 2024 12:46:13 +0000   Sun, 28 Jan 2024 12:42:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 28 Jan 2024 12:46:13 +0000   Sun, 28 Jan 2024 12:42:36 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.39.62
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             5925580Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             5925580Ki
  pods:               110
System Info:
  Machine ID:                 206b56ea609945ec95d2594d60906211
  System UUID:                206b56ea-6099-45ec-95d2-594d60906211
  Boot ID:                    a2286d24-47c7-4781-96bb-35a8701fad97
  Kernel Version:             5.10.57
  OS Image:                   Buildroot 2021.02.12
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  porto://5.3.30-alpha.9
  Kubelet Version:            v1.28.4
  Kube-Proxy Version:         v1.28.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 cilium-operator-7d98fd79fc-t9sl9    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m27s
  kube-system                 cilium-zxb49                        100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         3m27s
  kube-system                 coredns-5dd5756b68-9fmgs            100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (2%!)(MISSING)     3m27s
  kube-system                 etcd-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         3m40s
  kube-system                 kube-apiserver-minikube             250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m40s
  kube-system                 kube-controller-manager-minikube    200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m40s
  kube-system                 kube-proxy-97nfh                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m27s
  kube-system                 kube-scheduler-minikube             100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m40s
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3m39s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (42%!)(MISSING)  0 (0%!)(MISSING)
  memory             270Mi (4%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age    From             Message
  ----    ------                   ----   ----             -------
  Normal  Starting                 3m25s  kube-proxy       
  Normal  Starting                 3m40s  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  3m40s  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  3m40s  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m40s  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m40s  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           3m27s  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jan28 12:41] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000001] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000000] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +1.886287] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +0.500229] systemd-fstab-generator[113]: Ignoring "noauto" for root device
[  +0.028436] systemd[1]: systemd-journald.service: unit configures an IP firewall, but the local system does not support BPF/cgroup firewalling.
[  +0.000001] systemd[1]: (This warning is only shown for the first unit using IP firewalling.)
[  +6.713081] nbd: unknown parameter 'max_parts' ignored
[  +1.060824] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000003] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000001] NFSD: Unable to initialize client recovery tracking! (-2)
[Jan28 12:42] systemd-fstab-generator[631]: Ignoring "noauto" for root device
[  +0.044188] systemd-fstab-generator[642]: Ignoring "noauto" for root device
[  +0.541023] systemd-fstab-generator[809]: Ignoring "noauto" for root device
[  +0.037762] systemd-fstab-generator[820]: Ignoring "noauto" for root device
[  +1.051586] systemd-fstab-generator[834]: Ignoring "noauto" for root device
[  +0.047835] systemd-fstab-generator[845]: Ignoring "noauto" for root device
[  +0.053644] systemd-fstab-generator[863]: Ignoring "noauto" for root device
[ +25.362191] systemd-fstab-generator[1178]: Ignoring "noauto" for root device
[  +1.554468] systemd-fstab-generator[1398]: Ignoring "noauto" for root device
[  +0.212445] kauditd_printk_skb: 14 callbacks suppressed
[  +4.391973] systemd-fstab-generator[1517]: Ignoring "noauto" for root device
[  +0.795569] tee (1568): /proc/1467/oom_adj is deprecated, please use /proc/1467/oom_score_adj instead.
[ +13.204428] kauditd_printk_skb: 8 callbacks suppressed
[  +6.659275] kauditd_printk_skb: 37 callbacks suppressed
[Jan28 12:43] kauditd_printk_skb: 853 callbacks suppressed
[ +21.031524] kauditd_printk_skb: 852 callbacks suppressed
[Jan28 12:44] kauditd_printk_skb: 852 callbacks suppressed
[ +53.008128] kauditd_printk_skb: 874 callbacks suppressed


==> etcd [etcd-minikube-ba37/etcd-d578] <==
{"level":"warn","ts":"2024-01-28T12:42:34.83389Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-28T12:42:34.834025Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["/usr/local/bin/etcd","--advertise-client-urls=https://192.168.39.62:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.39.62:2380","--initial-cluster=minikube=https://192.168.39.62:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.39.62:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.39.62:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2024-01-28T12:42:34.834071Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-01-28T12:42:34.834083Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.39.62:2380"]}
{"level":"info","ts":"2024-01-28T12:42:34.834108Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-28T12:42:34.834412Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.39.62:2379"]}
{"level":"info","ts":"2024-01-28T12:42:34.834491Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.39.62:2380"],"listen-peer-urls":["https://192.168.39.62:2380"],"advertise-client-urls":["https://192.168.39.62:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.39.62:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.39.62:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-01-28T12:42:34.839062Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"4.434275ms"}
{"level":"info","ts":"2024-01-28T12:42:34.848714Z","caller":"etcdserver/raft.go:495","msg":"starting local member","local-member-id":"4cff10f3f970b356","cluster-id":"cebe0b560c7f0a8"}
{"level":"info","ts":"2024-01-28T12:42:34.848736Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 switched to configuration voters=()"}
{"level":"info","ts":"2024-01-28T12:42:34.848745Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 became follower at term 0"}
{"level":"info","ts":"2024-01-28T12:42:34.84875Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 4cff10f3f970b356 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2024-01-28T12:42:34.848753Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 became follower at term 1"}
{"level":"info","ts":"2024-01-28T12:42:34.84877Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 switched to configuration voters=(5548171905991750486)"}
{"level":"warn","ts":"2024-01-28T12:42:34.853759Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-01-28T12:42:34.855025Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-01-28T12:42:34.856218Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-01-28T12:42:34.85742Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"4cff10f3f970b356","local-server-version":"3.5.9","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-01-28T12:42:34.858009Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-28T12:42:34.858064Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"4cff10f3f970b356","initial-advertise-peer-urls":["https://192.168.39.62:2380"],"listen-peer-urls":["https://192.168.39.62:2380"],"advertise-client-urls":["https://192.168.39.62:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.39.62:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-01-28T12:42:34.858072Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-01-28T12:42:34.858194Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"4cff10f3f970b356","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-01-28T12:42:34.858258Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T12:42:34.858282Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T12:42:34.85829Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T12:42:34.858459Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.39.62:2380"}
{"level":"info","ts":"2024-01-28T12:42:34.858477Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.39.62:2380"}
{"level":"info","ts":"2024-01-28T12:42:34.858723Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 switched to configuration voters=(5548171905991750486)"}
{"level":"info","ts":"2024-01-28T12:42:34.858749Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"cebe0b560c7f0a8","local-member-id":"4cff10f3f970b356","added-peer-id":"4cff10f3f970b356","added-peer-peer-urls":["https://192.168.39.62:2380"]}
{"level":"info","ts":"2024-01-28T12:42:35.549721Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 is starting a new election at term 1"}
{"level":"info","ts":"2024-01-28T12:42:35.549744Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 became pre-candidate at term 1"}
{"level":"info","ts":"2024-01-28T12:42:35.549753Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 received MsgPreVoteResp from 4cff10f3f970b356 at term 1"}
{"level":"info","ts":"2024-01-28T12:42:35.549764Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 became candidate at term 2"}
{"level":"info","ts":"2024-01-28T12:42:35.549767Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 received MsgVoteResp from 4cff10f3f970b356 at term 2"}
{"level":"info","ts":"2024-01-28T12:42:35.549771Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"4cff10f3f970b356 became leader at term 2"}
{"level":"info","ts":"2024-01-28T12:42:35.549775Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 4cff10f3f970b356 elected leader 4cff10f3f970b356 at term 2"}
{"level":"info","ts":"2024-01-28T12:42:35.551913Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"4cff10f3f970b356","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.39.62:2379]}","request-path":"/0/members/4cff10f3f970b356/attributes","cluster-id":"cebe0b560c7f0a8","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-28T12:42:35.552011Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T12:42:35.552026Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T12:42:35.552812Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-01-28T12:42:35.552862Z","caller":"etcdserver/server.go:2571","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-28T12:42:35.552929Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-28T12:42:35.552937Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-28T12:42:35.55367Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"cebe0b560c7f0a8","local-member-id":"4cff10f3f970b356","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-28T12:42:35.553742Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-28T12:42:35.553752Z","caller":"etcdserver/server.go:2595","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-28T12:42:35.552814Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.39.62:2379"}


==> kernel <==
 12:46:18 up 4 min,  0 users,  load average: 0.19, 0.11, 0.03
Linux minikube 5.10.57 #1 SMP Thu Jan 18 13:11:25 MSK 2024 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"


==> kube-apiserver [kube-apiserver-minikube-ee59/kube-apiserver-1f90] <==
I0128 12:42:36.054270       6 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0128 12:42:36.054279       6 crdregistration_controller.go:111] Starting crd-autoregister controller
I0128 12:42:36.054282       6 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0128 12:42:36.054294       6 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0128 12:42:36.054319       6 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0128 12:42:36.057859       6 customresource_discovery_controller.go:289] Starting DiscoveryController
I0128 12:42:36.057875       6 system_namespaces_controller.go:67] Starting system namespaces controller
I0128 12:42:36.057933       6 gc_controller.go:78] Starting apiserver lease garbage collector
I0128 12:42:36.057982       6 controller.go:134] Starting OpenAPI controller
I0128 12:42:36.057991       6 controller.go:85] Starting OpenAPI V3 controller
I0128 12:42:36.057999       6 naming_controller.go:291] Starting NamingConditionController
I0128 12:42:36.058006       6 establishing_controller.go:76] Starting EstablishingController
I0128 12:42:36.058015       6 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0128 12:42:36.058020       6 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0128 12:42:36.058025       6 crd_finalizer.go:266] Starting CRDFinalizer
I0128 12:42:36.058238       6 controller.go:80] Starting OpenAPI V3 AggregationController
I0128 12:42:36.058298       6 gc_controller.go:78] Starting apiserver lease garbage collector
I0128 12:42:36.153992       6 apf_controller.go:377] Running API Priority and Fairness config worker
I0128 12:42:36.153999       6 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0128 12:42:36.154035       6 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0128 12:42:36.154047       6 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0128 12:42:36.154055       6 shared_informer.go:318] Caches are synced for configmaps
I0128 12:42:36.154396       6 shared_informer.go:318] Caches are synced for crd-autoregister
I0128 12:42:36.154429       6 cache.go:39] Caches are synced for AvailableConditionController controller
I0128 12:42:36.154441       6 aggregator.go:166] initial CRD sync complete...
I0128 12:42:36.154452       6 autoregister_controller.go:141] Starting autoregister controller
I0128 12:42:36.154459       6 cache.go:32] Waiting for caches to sync for autoregister controller
I0128 12:42:36.154466       6 cache.go:39] Caches are synced for autoregister controller
I0128 12:42:36.156160       6 controller.go:624] quota admission added evaluator for: namespaces
I0128 12:42:36.156694       6 shared_informer.go:318] Caches are synced for node_authorizer
E0128 12:42:36.160922       6 controller.go:146] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0128 12:42:36.362557       6 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0128 12:42:37.060543       6 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0128 12:42:37.062356       6 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0128 12:42:37.062486       6 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0128 12:42:37.299971       6 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0128 12:42:37.315547       6 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0128 12:42:37.362219       6 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0128 12:42:37.365034       6 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.39.62]
I0128 12:42:37.365424       6 controller.go:624] quota admission added evaluator for: endpoints
I0128 12:42:37.367561       6 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0128 12:42:38.090926       6 controller.go:624] quota admission added evaluator for: serviceaccounts
I0128 12:42:38.814496       6 controller.go:624] quota admission added evaluator for: deployments.apps
I0128 12:42:38.821460       6 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0128 12:42:38.828170       6 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0128 12:42:51.294079       6 controller.go:624] quota admission added evaluator for: replicasets.apps
I0128 12:42:51.792625       6 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0128 12:43:03.175298       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.182118       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.185206       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.189552       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.194188       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.196788       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.209428       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.236300       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.279853       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.312176       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.593261       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.640892       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager
I0128 12:43:03.671922       6 handler.go:232] Adding GroupVersion cilium.io v2 to ResourceManager


==> kube-controller-manager [kube-controller-manager-minikube-0889/kube-controller-manager-4349] <==
I0128 12:42:51.339215       6 shared_informer.go:318] Caches are synced for TTL
I0128 12:42:51.339249       6 shared_informer.go:318] Caches are synced for taint
I0128 12:42:51.339280       6 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0128 12:42:51.339334       6 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0128 12:42:51.339362       6 taint_manager.go:205] "Starting NoExecuteTaintManager"
I0128 12:42:51.339378       6 taint_manager.go:210] "Sending events to api server"
I0128 12:42:51.339556       6 shared_informer.go:318] Caches are synced for service account
I0128 12:42:51.340014       6 shared_informer.go:318] Caches are synced for daemon sets
I0128 12:42:51.340105       6 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0128 12:42:51.340161       6 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0128 12:42:51.340203       6 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0128 12:42:51.340344       6 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0128 12:42:51.342285       6 shared_informer.go:318] Caches are synced for cronjob
I0128 12:42:51.342989       6 shared_informer.go:318] Caches are synced for TTL after finished
I0128 12:42:51.344124       6 shared_informer.go:318] Caches are synced for node
I0128 12:42:51.344155       6 range_allocator.go:174] "Sending events to api server"
I0128 12:42:51.344165       6 range_allocator.go:178] "Starting range CIDR allocator"
I0128 12:42:51.344168       6 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0128 12:42:51.344171       6 shared_informer.go:318] Caches are synced for cidrallocator
I0128 12:42:51.348092       6 shared_informer.go:318] Caches are synced for persistent volume
I0128 12:42:51.350967       6 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5dd5756b68-9fmgs"
I0128 12:42:51.351007       6 shared_informer.go:318] Caches are synced for ReplicationController
I0128 12:42:51.352366       6 event.go:307] "Event occurred" object="kube-system/cilium-operator-7d98fd79fc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cilium-operator-7d98fd79fc-t9sl9"
I0128 12:42:51.358475       6 range_allocator.go:380] "Set node PodCIDR" node="minikube" podCIDRs=["10.244.0.0/24"]
I0128 12:42:51.361379       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="41.573157ms"
I0128 12:42:51.362538       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/cilium-operator-7d98fd79fc" duration="42.765547ms"
I0128 12:42:51.367931       6 shared_informer.go:318] Caches are synced for namespace
I0128 12:42:51.368186       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="6.789634ms"
I0128 12:42:51.368250       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="19.677¬µs"
I0128 12:42:51.370530       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/cilium-operator-7d98fd79fc" duration="7.954921ms"
I0128 12:42:51.370664       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/cilium-operator-7d98fd79fc" duration="14.839¬µs"
I0128 12:42:51.370992       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/cilium-operator-7d98fd79fc" duration="96.401¬µs"
I0128 12:42:51.371969       6 shared_informer.go:318] Caches are synced for PV protection
I0128 12:42:51.388793       6 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0128 12:42:51.388952       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="17.964¬µs"
I0128 12:42:51.395884       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/cilium-operator-7d98fd79fc" duration="26.23¬µs"
I0128 12:42:51.408912       6 shared_informer.go:318] Caches are synced for endpoint_slice
I0128 12:42:51.439470       6 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0128 12:42:51.439544       6 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0128 12:42:51.439476       6 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0128 12:42:51.440370       6 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0128 12:42:51.489371       6 shared_informer.go:318] Caches are synced for HPA
I0128 12:42:51.560836       6 shared_informer.go:318] Caches are synced for resource quota
I0128 12:42:51.572470       6 shared_informer.go:318] Caches are synced for resource quota
I0128 12:42:51.799565       6 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-97nfh"
I0128 12:42:51.802087       6 event.go:307] "Event occurred" object="kube-system/cilium" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: cilium-zxb49"
I0128 12:42:51.888087       6 shared_informer.go:318] Caches are synced for garbage collector
I0128 12:42:51.889232       6 shared_informer.go:318] Caches are synced for garbage collector
I0128 12:42:51.889261       6 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0128 12:42:53.963914       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="130.908¬µs"
I0128 12:42:53.990235       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="8.465768ms"
I0128 12:42:53.990340       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="21.249¬µs"
I0128 12:43:04.002285       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/cilium-operator-7d98fd79fc" duration="4.440888ms"
I0128 12:43:04.002333       6 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/cilium-operator-7d98fd79fc" duration="34.042¬µs"
I0128 12:43:21.579666       6 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ciliumendpoints.cilium.io"
I0128 12:43:21.579755       6 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="ciliumnetworkpolicies.cilium.io"
I0128 12:43:21.579796       6 shared_informer.go:311] Waiting for caches to sync for resource quota
I0128 12:43:21.680748       6 shared_informer.go:318] Caches are synced for resource quota
I0128 12:43:21.892688       6 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0128 12:43:21.993299       6 shared_informer.go:318] Caches are synced for garbage collector


==> kube-proxy [kube-proxy-97nfh-8f2d/kube-proxy-321c] <==
I0128 12:42:52.758060       6 server_others.go:69] "Using iptables proxy"
I0128 12:42:52.762288       6 node.go:141] Successfully retrieved node IP: 192.168.39.62
I0128 12:42:52.769510       6 server_others.go:121] "No iptables support for family" ipFamily="IPv6"
I0128 12:42:52.769518       6 server.go:634] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0128 12:42:52.770068       6 server_others.go:152] "Using iptables Proxier"
I0128 12:42:52.770083       6 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0128 12:42:52.770209       6 server.go:846] "Version info" version="v1.28.4"
I0128 12:42:52.770215       6 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 12:42:52.770462       6 config.go:188] "Starting service config controller"
I0128 12:42:52.770472       6 shared_informer.go:311] Waiting for caches to sync for service config
I0128 12:42:52.770483       6 config.go:97] "Starting endpoint slice config controller"
I0128 12:42:52.770486       6 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0128 12:42:52.770640       6 config.go:315] "Starting node config controller"
I0128 12:42:52.770644       6 shared_informer.go:311] Waiting for caches to sync for node config
I0128 12:42:52.871119       6 shared_informer.go:318] Caches are synced for node config
I0128 12:42:52.871128       6 shared_informer.go:318] Caches are synced for service config
I0128 12:42:52.871150       6 shared_informer.go:318] Caches are synced for endpoint slice config


==> kube-scheduler [kube-scheduler-minikube-463a/kube-scheduler-9b75] <==
I0128 12:42:35.188954       7 serving.go:348] Generated self-signed cert in-memory
W0128 12:42:36.090939       7 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0128 12:42:36.090952       7 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0128 12:42:36.090957       7 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0128 12:42:36.090961       7 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0128 12:42:36.105977       7 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.4"
I0128 12:42:36.105987       7 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 12:42:36.106678       7 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0128 12:42:36.106726       7 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0128 12:42:36.106733       7 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0128 12:42:36.106741       7 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0128 12:42:36.113721       7 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0128 12:42:36.113735       7 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0128 12:42:36.113799       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0128 12:42:36.113806       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0128 12:42:36.113825       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0128 12:42:36.113828       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0128 12:42:36.113847       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0128 12:42:36.113851       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0128 12:42:36.113876       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0128 12:42:36.113879       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0128 12:42:36.113895       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0128 12:42:36.113900       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0128 12:42:36.113918       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0128 12:42:36.113924       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0128 12:42:36.113946       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0128 12:42:36.113951       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0128 12:42:36.113966       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0128 12:42:36.113969       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0128 12:42:36.113986       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0128 12:42:36.113991       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0128 12:42:36.114012       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0128 12:42:36.114017       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0128 12:42:36.114033       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0128 12:42:36.114038       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0128 12:42:36.114053       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0128 12:42:36.114058       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0128 12:42:36.114072       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0128 12:42:36.114082       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0128 12:42:36.114095       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0128 12:42:36.114100       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0128 12:42:36.970905       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0128 12:42:36.970925       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0128 12:42:37.144679       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0128 12:42:37.144695       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0128 12:42:37.213682       7 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0128 12:42:37.213697       7 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0128 12:42:37.257627       7 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0128 12:42:37.257642       7 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0128 12:42:40.007026       7 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
-- Journal begins at Sun 2024-01-28 12:41:51 UTC, ends at Sun 2024-01-28 12:46:18 UTC. --
Jan 28 12:45:02 minikube kubelet[1524]: I0128 12:45:02.288174    1524 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 28 12:45:02 minikube kubelet[1524]: E0128 12:45:02.288774    1524 kubelet_node_status.go:597] "Error updating pod CIDR" err="failed to update pod CIDR: rpc error: code = Unknown desc = not implemented UpdateRuntimeConfig"
Jan 28 12:45:07 minikube kubelet[1524]: I0128 12:45:07.872661    1524 scope.go:117] "RemoveContainer" containerID="cilium-zxb49-7d4d/cilium-agent-7a51"
Jan 28 12:45:07 minikube kubelet[1524]: E0128 12:45:07.872904    1524 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cilium-agent\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cilium-agent pod=cilium-zxb49_kube-system(2ea7885c-586c-4004-9aa6-33b484bfef7c)\"" pod="kube-system/cilium-zxb49" podUID="2ea7885c-586c-4004-9aa6-33b484bfef7c"
Jan 28 12:45:09 minikube kubelet[1524]: E0128 12:45:09.032856    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c\": with major: 0, minor: 189: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c"
Jan 28 12:45:09 minikube kubelet[1524]: E0128 12:45:09.032897    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca\": with major: 0, minor: 173: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
Jan 28 12:45:09 minikube kubelet[1524]: E0128 12:45:09.032971    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c\": with major: 0, minor: 130: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c"
Jan 28 12:45:09 minikube kubelet[1524]: E0128 12:45:09.033004    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961\": with major: 0, minor: 159: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961"
Jan 28 12:45:12 minikube kubelet[1524]: I0128 12:45:12.458062    1524 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 28 12:45:12 minikube kubelet[1524]: E0128 12:45:12.458383    1524 kubelet_node_status.go:597] "Error updating pod CIDR" err="failed to update pod CIDR: rpc error: code = Unknown desc = not implemented UpdateRuntimeConfig"
Jan 28 12:45:14 minikube kubelet[1524]: I0128 12:45:14.928271    1524 scope.go:117] "RemoveContainer" containerID="cilium-zxb49-7d4d/cilium-agent-7a51"
Jan 28 12:45:14 minikube kubelet[1524]: E0128 12:45:14.928537    1524 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cilium-agent\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cilium-agent pod=cilium-zxb49_kube-system(2ea7885c-586c-4004-9aa6-33b484bfef7c)\"" pod="kube-system/cilium-zxb49" podUID="2ea7885c-586c-4004-9aa6-33b484bfef7c"
Jan 28 12:45:19 minikube kubelet[1524]: E0128 12:45:19.041173    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c\": with major: 0, minor: 189: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c"
Jan 28 12:45:19 minikube kubelet[1524]: E0128 12:45:19.041421    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca\": with major: 0, minor: 173: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
Jan 28 12:45:19 minikube kubelet[1524]: E0128 12:45:19.041513    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c\": with major: 0, minor: 130: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c"
Jan 28 12:45:19 minikube kubelet[1524]: E0128 12:45:19.041566    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961\": with major: 0, minor: 159: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961"
Jan 28 12:45:22 minikube kubelet[1524]: I0128 12:45:22.624172    1524 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 28 12:45:22 minikube kubelet[1524]: E0128 12:45:22.624679    1524 kubelet_node_status.go:597] "Error updating pod CIDR" err="failed to update pod CIDR: rpc error: code = Unknown desc = not implemented UpdateRuntimeConfig"
Jan 28 12:45:25 minikube kubelet[1524]: I0128 12:45:25.873081    1524 scope.go:117] "RemoveContainer" containerID="cilium-zxb49-7d4d/cilium-agent-7a51"
Jan 28 12:45:25 minikube kubelet[1524]: E0128 12:45:25.873330    1524 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cilium-agent\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cilium-agent pod=cilium-zxb49_kube-system(2ea7885c-586c-4004-9aa6-33b484bfef7c)\"" pod="kube-system/cilium-zxb49" podUID="2ea7885c-586c-4004-9aa6-33b484bfef7c"
Jan 28 12:45:29 minikube kubelet[1524]: E0128 12:45:29.049744    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c\": with major: 0, minor: 189: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c"
Jan 28 12:45:29 minikube kubelet[1524]: E0128 12:45:29.049779    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca\": with major: 0, minor: 173: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
Jan 28 12:45:29 minikube kubelet[1524]: E0128 12:45:29.049850    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c\": with major: 0, minor: 130: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c"
Jan 28 12:45:29 minikube kubelet[1524]: E0128 12:45:29.049882    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961\": with major: 0, minor: 159: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961"
Jan 28 12:45:32 minikube kubelet[1524]: I0128 12:45:32.808016    1524 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 28 12:45:32 minikube kubelet[1524]: E0128 12:45:32.808511    1524 kubelet_node_status.go:597] "Error updating pod CIDR" err="failed to update pod CIDR: rpc error: code = Unknown desc = not implemented UpdateRuntimeConfig"
Jan 28 12:45:38 minikube kubelet[1524]: I0128 12:45:38.873018    1524 scope.go:117] "RemoveContainer" containerID="cilium-zxb49-7d4d/cilium-agent-7a51"
Jan 28 12:45:38 minikube kubelet[1524]: E0128 12:45:38.873508    1524 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cilium-agent\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cilium-agent pod=cilium-zxb49_kube-system(2ea7885c-586c-4004-9aa6-33b484bfef7c)\"" pod="kube-system/cilium-zxb49" podUID="2ea7885c-586c-4004-9aa6-33b484bfef7c"
Jan 28 12:45:38 minikube kubelet[1524]: E0128 12:45:38.900605    1524 iptables.go:575] "Could not set up iptables canary" err=<
Jan 28 12:45:38 minikube kubelet[1524]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: ip6tables v1.8.6 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
Jan 28 12:45:38 minikube kubelet[1524]:         Perhaps ip6tables or your kernel needs to be upgraded.
Jan 28 12:45:38 minikube kubelet[1524]:  > table="nat" chain="KUBE-KUBELET-CANARY"
Jan 28 12:45:39 minikube kubelet[1524]: E0128 12:45:39.058284    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c\": with major: 0, minor: 189: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c"
Jan 28 12:45:39 minikube kubelet[1524]: E0128 12:45:39.058404    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca\": with major: 0, minor: 173: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
Jan 28 12:45:39 minikube kubelet[1524]: E0128 12:45:39.058489    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c\": with major: 0, minor: 130: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c"
Jan 28 12:45:39 minikube kubelet[1524]: E0128 12:45:39.058545    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961\": with major: 0, minor: 159: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961"
Jan 28 12:45:43 minikube kubelet[1524]: I0128 12:45:43.030316    1524 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 28 12:45:43 minikube kubelet[1524]: E0128 12:45:43.030878    1524 kubelet_node_status.go:597] "Error updating pod CIDR" err="failed to update pod CIDR: rpc error: code = Unknown desc = not implemented UpdateRuntimeConfig"
Jan 28 12:45:49 minikube kubelet[1524]: E0128 12:45:49.065878    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c\": with major: 0, minor: 189: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c"
Jan 28 12:45:49 minikube kubelet[1524]: E0128 12:45:49.066179    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca\": with major: 0, minor: 173: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
Jan 28 12:45:49 minikube kubelet[1524]: E0128 12:45:49.066302    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c\": with major: 0, minor: 130: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c"
Jan 28 12:45:49 minikube kubelet[1524]: E0128 12:45:49.066359    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961\": with major: 0, minor: 159: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961"
Jan 28 12:45:52 minikube kubelet[1524]: I0128 12:45:52.873042    1524 scope.go:117] "RemoveContainer" containerID="cilium-zxb49-7d4d/cilium-agent-7a51"
Jan 28 12:45:52 minikube kubelet[1524]: E0128 12:45:52.873595    1524 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cilium-agent\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cilium-agent pod=cilium-zxb49_kube-system(2ea7885c-586c-4004-9aa6-33b484bfef7c)\"" pod="kube-system/cilium-zxb49" podUID="2ea7885c-586c-4004-9aa6-33b484bfef7c"
Jan 28 12:45:53 minikube kubelet[1524]: I0128 12:45:53.148044    1524 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 28 12:45:53 minikube kubelet[1524]: E0128 12:45:53.148375    1524 kubelet_node_status.go:597] "Error updating pod CIDR" err="failed to update pod CIDR: rpc error: code = Unknown desc = not implemented UpdateRuntimeConfig"
Jan 28 12:45:59 minikube kubelet[1524]: E0128 12:45:59.074532    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c\": with major: 0, minor: 189: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c"
Jan 28 12:45:59 minikube kubelet[1524]: E0128 12:45:59.074783    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca\": with major: 0, minor: 173: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
Jan 28 12:45:59 minikube kubelet[1524]: E0128 12:45:59.074900    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c\": with major: 0, minor: 130: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c"
Jan 28 12:45:59 minikube kubelet[1524]: E0128 12:45:59.074955    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961\": with major: 0, minor: 159: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961"
Jan 28 12:46:03 minikube kubelet[1524]: I0128 12:46:03.399006    1524 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 28 12:46:03 minikube kubelet[1524]: E0128 12:46:03.399316    1524 kubelet_node_status.go:597] "Error updating pod CIDR" err="failed to update pod CIDR: rpc error: code = Unknown desc = not implemented UpdateRuntimeConfig"
Jan 28 12:46:04 minikube kubelet[1524]: I0128 12:46:04.873360    1524 scope.go:117] "RemoveContainer" containerID="cilium-zxb49-7d4d/cilium-agent-7a51"
Jan 28 12:46:04 minikube kubelet[1524]: E0128 12:46:04.873566    1524 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cilium-agent\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=cilium-agent pod=cilium-zxb49_kube-system(2ea7885c-586c-4004-9aa6-33b484bfef7c)\"" pod="kube-system/cilium-zxb49" podUID="2ea7885c-586c-4004-9aa6-33b484bfef7c"
Jan 28 12:46:09 minikube kubelet[1524]: E0128 12:46:09.082549    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c\": with major: 0, minor: 189: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/cilium-operator-7d98fd79fc-t9sl9-2dd8/cilium-operator-e33c"
Jan 28 12:46:09 minikube kubelet[1524]: E0128 12:46:09.082783    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca\": with major: 0, minor: 173: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/coredns-5dd5756b68-9fmgs-292a/coredns-0dca"
Jan 28 12:46:09 minikube kubelet[1524]: E0128 12:46:09.082870    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c\": with major: 0, minor: 130: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/kube-proxy-97nfh-8f2d/kube-proxy-321c"
Jan 28 12:46:09 minikube kubelet[1524]: E0128 12:46:09.082925    1524 cri_stats_provider.go:448] "Failed to get the info of the filesystem with mountpoint" err="failed to get device for dir \"/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961\": with major: 0, minor: 159: could not find device in cached partitions map" mountpoint="/place/portoshim_volumes/storage-provisioner-a4ae/storage-provisioner-1961"
Jan 28 12:46:13 minikube kubelet[1524]: I0128 12:46:13.427303    1524 kuberuntime_manager.go:1528] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 28 12:46:13 minikube kubelet[1524]: E0128 12:46:13.427795    1524 kubelet_node_status.go:597] "Error updating pod CIDR" err="failed to update pod CIDR: rpc error: code = Unknown desc = not implemented UpdateRuntimeConfig"


==> porto <==
2024-01-28 12:46:18.354 portod[1189]:     Connected CL26:portoshim(379) CT1:/ cred=root:root tcred=root:root access=rw ns= wns=
2024-01-28 12:46:18.354 portod-RO7[1262][4a4bad16]: REQ Version   from CL26:portoshim(379) CT1:/
2024-01-28 12:46:18.354 portod-RO7[1262][4a4bad16]: DBG Raw request: version { }
2024-01-28 12:46:18.354 portod-RO7[1262][4a4bad16]: RSP Version   5.3.30-alpha.9 # to CL26:portoshim(379) CT1:/ lock=0 ms time=0+0 ms
2024-01-28 12:46:18.354 portod-RO7[1262][4a4bad16]: DBG Raw response: error: Success errorMsg: "" version { tag: "5.3.30-alpha.9" revision: "" } timestamp: 1706445978
2024-01-28 12:46:18.355 portod[1189]:     Connected CL28:portoshim(379) CT1:/ cred=root:root tcred=root:root access=rw ns= wns=
2024-01-28 12:46:18.355 portod-RO5[1260][5e454b98]: REQ Get   etcd-minikube-ba37/etcd-d578 -- labels state creation_time[raw] start_time[raw] death_time[raw] exit_code from CL28:portoshim(379) CT1:/
2024-01-28 12:46:18.355 portod-RO5[1260][5e454b98]: DBG Raw request: get { name: "etcd-minikube-ba37/etcd-d578" variable: "labels" variable: "state" variable: "creation_time[raw]" variable: "start_time[raw]" variable: "death_time[raw]" variable: "exit_code" }
2024-01-28 12:46:18.355 portod-RO5[1260][5e454b98]: DBG LockStateRead CT10:etcd-minikube-ba37/etcd-d578
2024-01-28 12:46:18.355 portod-RO5[1260][5e454b98]: DBG UnlockState CT10:etcd-minikube-ba37/etcd-d578
2024-01-28 12:46:18.355 portod-RO5[1260][5e454b98]: RSP Get   etcd-minikube-ba37/etcd-d578 -- labels state creation_time[raw] start_time[raw] death_time[raw] exit_code etcd-minikube-ba37/etcd-d578: labels=ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIuaGFzaA: YzVhNTk0NDU; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIucmVzdGFydENvdW50: MA; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUG9saWN5: RmlsZQ; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUGF0aA: L2Rldi90ZXJtaW5hdGlvbi1sb2c; ANNOTATION.aW8ua3ViZXJuZXRlcy5wb2QudGVybWluYXRpb25HcmFjZVBlcmlvZA: MzA; LABEL.YXR0ZW1wdA: MA; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubG9ncGF0aA: L3BsYWNlL3BvcnRvL2V0Y2QtbWluaWt1YmUtYmEzNy9ldGNkLWQ1Nzgvc3Rkb3V0; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubmFtZQ: ZXRjZA; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZQ: ZXRjZC1taW5pa3ViZQ; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZXNwYWNl: a3ViZS1zeXN0ZW0; LABEL.aW8ua3ViZXJuZXRlcy5wb2QudWlk: MzIzZjc3YTAzM2UzYzRmNDE5MmJmOThhMGFjYjg3YmY; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pZA: ZXRjZC1taW5pa3ViZS1iYTM3L2V0Y2QtZDU3OA; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pbWFnZQ: NzNkZWI5YTNmNzAyNTMyNTkyYTQxNjc0NTVmOGJmMmU1ZjVkOTAwYmNjOTU5YmEyZmQyZDM1YzMyMWRlMWFmOQ state=running creation_time[raw]=1706445754 start_time[raw]=1706445754 death_time[raw]=8:InvalidState(death_time available only in dead state) exit_code=8:InvalidState(exit_code available only in dead state) to CL28:portoshim(379) CT1:/ lock=0 ms time=0+0 ms
2024-01-28 12:46:18.355 portod-RO5[1260][5e454b98]: DBG Raw response: error: Success errorMsg: "" get { list { name: "etcd-minikube-ba37/etcd-d578" keyval { variable: "labels" value: "ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIuaGFzaA: YzVhNTk0NDU; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIucmVzdGFydENvdW50: MA; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUG9saWN5: RmlsZQ; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUGF0aA: L2Rldi90ZXJtaW5hdGlvbi1sb2c; ANNOTATION.aW8ua3ViZXJuZXRlcy5wb2QudGVybWluYXRpb25HcmFjZVBlcmlvZA: MzA; LABEL.YXR0ZW1wdA: MA; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubG9ncGF0aA: L3BsYWNlL3BvcnRvL2V0Y2QtbWluaWt1YmUtYmEzNy9ldGNkLWQ1Nzgvc3Rkb3V0; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubmFtZQ: ZXRjZA; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZQ: ZXRjZC1taW5pa3ViZQ; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZXNwYWNl: a3ViZS1zeXN0ZW0; LABEL.aW8ua3ViZXJuZXRlcy5wb2QudWlk: MzIzZjc3YTAzM2UzYzRmNDE5MmJmOThhMGFjYjg3YmY; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pZA: ZXRjZC1taW5pa3ViZS1iYTM3L2V0Y2QtZDU3OA; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pbWFnZQ: NzNkZWI5YTNmNzAyNTMyNTkyYTQxNjc0NTVmOGJmMmU1ZjVkOTAwYmNjOTU5YmEyZmQyZDM1YzMyMWRlMWFmOQ" } keyval { variable: "state" value: "running" } keyval { variable: "creation_time[raw]" value: "1706445754" } keyval { variable: "start_time[raw]" value: "1706445754" } keyval { variable: "death_time[raw]" error: InvalidState errorMsg: "death_time available only in dead state" } keyval { variable: "exit_code" error: InvalidState errorMsg: "exit_code available only in dead state" } change_time: 1706445754 } } timestamp: 1706445978
2024-01-28 12:46:18.377 portod[1189]:     Connected CL30:portoshim(379) CT1:/ cred=root:root tcred=root:root access=rw ns= wns=
2024-01-28 12:46:18.377 portod-RO6[1261][6d16a347]: REQ Version   from CL30:portoshim(379) CT1:/
2024-01-28 12:46:18.377 portod-RO6[1261][6d16a347]: DBG Raw request: version { }
2024-01-28 12:46:18.377 portod-RO6[1261][6d16a347]: RSP Version   5.3.30-alpha.9 # to CL30:portoshim(379) CT1:/ lock=0 ms time=0+0 ms
2024-01-28 12:46:18.377 portod-RO6[1261][6d16a347]: DBG Raw response: error: Success errorMsg: "" version { tag: "5.3.30-alpha.9" revision: "" } timestamp: 1706445978
2024-01-28 12:46:18.377 portod[1189]:     Connected CL33:portoshim(379) CT1:/ cred=root:root tcred=root:root access=rw ns= wns=
2024-01-28 12:46:18.377 portod-RO3[1258][0b4c81a7]: REQ Get   kube-apiserver-minikube-ee59/kube-apiserver-1f90 -- labels state creation_time[raw] start_time[raw] death_time[raw] exit_code from CL33:portoshim(379) CT1:/
2024-01-28 12:46:18.377 portod-RO3[1258][0b4c81a7]: DBG Raw request: get { name: "kube-apiserver-minikube-ee59/kube-apiserver-1f90" variable: "labels" variable: "state" variable: "creation_time[raw]" variable: "start_time[raw]" variable: "death_time[raw]" variable: "exit_code" }
2024-01-28 12:46:18.377 portod-RO3[1258][0b4c81a7]: DBG LockStateRead CT8:kube-apiserver-minikube-ee59/kube-apiserver-1f90
2024-01-28 12:46:18.377 portod-RO3[1258][0b4c81a7]: DBG UnlockState CT8:kube-apiserver-minikube-ee59/kube-apiserver-1f90
2024-01-28 12:46:18.377 portod-RO3[1258][0b4c81a7]: RSP Get   kube-apiserver-minikube-ee59/kube-apiserver-1f90 -- labels state creation_time[raw] start_time[raw] death_time[raw] exit_code kube-apiserver-minikube-ee59/kube-apiserver-1f90: labels=ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIuaGFzaA: MmY3MTk3OGU; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIucmVzdGFydENvdW50: MA; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUG9saWN5: RmlsZQ; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUGF0aA: L2Rldi90ZXJtaW5hdGlvbi1sb2c; ANNOTATION.aW8ua3ViZXJuZXRlcy5wb2QudGVybWluYXRpb25HcmFjZVBlcmlvZA: MzA; LABEL.YXR0ZW1wdA: MA; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubG9ncGF0aA: L3BsYWNlL3BvcnRvL2t1YmUtYXBpc2VydmVyLW1pbmlrdWJlLWVlNTkva3ViZS1hcGlzZXJ2ZXItMWY5MC9zdGRvdXQ; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubmFtZQ: a3ViZS1hcGlzZXJ2ZXI; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZQ: a3ViZS1hcGlzZXJ2ZXItbWluaWt1YmU; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZXNwYWNl: a3ViZS1zeXN0ZW0; LABEL.aW8ua3ViZXJuZXRlcy5wb2QudWlk: ZjgzYmQzYmY3NjI5NDI5MWI4ZTVhNWY1MDE3MmU0NmI; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pZA: a3ViZS1hcGlzZXJ2ZXItbWluaWt1YmUtZWU1OS9rdWJlLWFwaXNlcnZlci0xZjkw; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pbWFnZQ: N2ZlMGU2ZjM3ZGIzMzQ2NDcyNWU2MTZhMTJjY2M0ZTM2OTcwMzcwMDA1YTJiMDk2ODNhOTc0ZGI2MzUwYzI1Nw state=running creation_time[raw]=1706445754 start_time[raw]=1706445754 death_time[raw]=8:InvalidState(death_time available only in dead state) exit_code=8:InvalidState(exit_code available only in dead state) to CL33:portoshim(379) CT1:/ lock=0 ms time=0+0 ms
2024-01-28 12:46:18.377 portod-RO3[1258][0b4c81a7]: DBG Raw response: error: Success errorMsg: "" get { list { name: "kube-apiserver-minikube-ee59/kube-apiserver-1f90" keyval { variable: "labels" value: "ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIuaGFzaA: MmY3MTk3OGU; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIucmVzdGFydENvdW50: MA; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUG9saWN5: RmlsZQ; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUGF0aA: L2Rldi90ZXJtaW5hdGlvbi1sb2c; ANNOTATION.aW8ua3ViZXJuZXRlcy5wb2QudGVybWluYXRpb25HcmFjZVBlcmlvZA: MzA; LABEL.YXR0ZW1wdA: MA; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubG9ncGF0aA: L3BsYWNlL3BvcnRvL2t1YmUtYXBpc2VydmVyLW1pbmlrdWJlLWVlNTkva3ViZS1hcGlzZXJ2ZXItMWY5MC9zdGRvdXQ; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubmFtZQ: a3ViZS1hcGlzZXJ2ZXI; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZQ: a3ViZS1hcGlzZXJ2ZXItbWluaWt1YmU; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZXNwYWNl: a3ViZS1zeXN0ZW0; LABEL.aW8ua3ViZXJuZXRlcy5wb2QudWlk: ZjgzYmQzYmY3NjI5NDI5MWI4ZTVhNWY1MDE3MmU0NmI; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pZA: a3ViZS1hcGlzZXJ2ZXItbWluaWt1YmUtZWU1OS9rdWJlLWFwaXNlcnZlci0xZjkw; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pbWFnZQ: N2ZlMGU2ZjM3ZGIzMzQ2NDcyNWU2MTZhMTJjY2M0ZTM2OTcwMzcwMDA1YTJiMDk2ODNhOTc0ZGI2MzUwYzI1Nw" } keyval { variable: "state" value: "running" } keyval { variable: "creation_time[raw]" value: "1706445754" } keyval { variable: "start_time[raw]" value: "1706445754" } keyval { variable: "death_time[raw]" error: InvalidState errorMsg: "death_time available only in dead state" } keyval { variable: "exit_code" error: InvalidState errorMsg: "exit_code available only in dead state" } change_time: 1706445754 } } timestamp: 1706445978
2024-01-28 12:46:18.392 portod[1189]:     Connected CL37:portoshim(379) CT1:/ cred=root:root tcred=root:root access=rw ns= wns=
2024-01-28 12:46:18.392 portod-RO2[1257][407a13c3]: REQ Version   from CL37:portoshim(379) CT1:/
2024-01-28 12:46:18.392 portod-RO2[1257][407a13c3]: DBG Raw request: version { }
2024-01-28 12:46:18.392 portod-RO2[1257][407a13c3]: RSP Version   5.3.30-alpha.9 # to CL37:portoshim(379) CT1:/ lock=0 ms time=0+0 ms
2024-01-28 12:46:18.392 portod-RO2[1257][407a13c3]: DBG Raw response: error: Success errorMsg: "" version { tag: "5.3.30-alpha.9" revision: "" } timestamp: 1706445978
2024-01-28 12:46:18.392 portod[1189]:     Connected CL38:portoshim(379) CT1:/ cred=root:root tcred=root:root access=rw ns= wns=
2024-01-28 12:46:18.392 portod-RO9[1264][26117ebe]: REQ Get   kube-controller-manager-minikube-0889/kube-controller-manager-4349 -- labels state creation_time[raw] start_time[raw] death_time[raw] exit_code from CL38:portoshim(379) CT1:/
2024-01-28 12:46:18.392 portod-RO9[1264][26117ebe]: DBG Raw request: get { name: "kube-controller-manager-minikube-0889/kube-controller-manager-4349" variable: "labels" variable: "state" variable: "creation_time[raw]" variable: "start_time[raw]" variable: "death_time[raw]" variable: "exit_code" }
2024-01-28 12:46:18.392 portod-RO9[1264][26117ebe]: DBG LockStateRead CT11:kube-controller-manager-minikube-0889/kube-controller-manager-4349
2024-01-28 12:46:18.392 portod-RO9[1264][26117ebe]: DBG UnlockState CT11:kube-controller-manager-minikube-0889/kube-controller-manager-4349
2024-01-28 12:46:18.392 portod-RO9[1264][26117ebe]: RSP Get   kube-controller-manager-minikube-0889/kube-controller-manager-4349 -- labels state creation_time[raw] start_time[raw] death_time[raw] exit_code kube-controller-manager-minikube-0889/kube-controller-manager-4349: labels=ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIuaGFzaA: NGI5YzUxZmM; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIucmVzdGFydENvdW50: MA; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUG9saWN5: RmlsZQ; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUGF0aA: L2Rldi90ZXJtaW5hdGlvbi1sb2c; ANNOTATION.aW8ua3ViZXJuZXRlcy5wb2QudGVybWluYXRpb25HcmFjZVBlcmlvZA: MzA; LABEL.YXR0ZW1wdA: MA; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubG9ncGF0aA: L3BsYWNlL3BvcnRvL2t1YmUtY29udHJvbGxlci1tYW5hZ2VyLW1pbmlrdWJlLTA4ODkva3ViZS1jb250cm9sbGVyLW1hbmFnZXItNDM0OS9zdGRvdXQ; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubmFtZQ: a3ViZS1jb250cm9sbGVyLW1hbmFnZXI; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZQ: a3ViZS1jb250cm9sbGVyLW1hbmFnZXItbWluaWt1YmU; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZXNwYWNl: a3ViZS1zeXN0ZW0; LABEL.aW8ua3ViZXJuZXRlcy5wb2QudWlk: YzcwNTYzN2Q3MGE0YWE0NGU2MDc4NTlmMDA2YzI2ZjA; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pZA: a3ViZS1jb250cm9sbGVyLW1hbmFnZXItbWluaWt1YmUtMDg4OS9rdWJlLWNvbnRyb2xsZXItbWFuYWdlci00MzQ5; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pbWFnZQ: ZDA1OGFhNWFiOTY5Y2U3Yjg0ZDI1ZTcxODhiZTFmODA2MzNiMThkYjhlYTdkMDJkOWQwYTc4ZTY3NjIzNjU5MQ state=running creation_time[raw]=1706445754 start_time[raw]=1706445754 death_time[raw]=8:InvalidState(death_time available only in dead state) exit_code=8:InvalidState(exit_code available only in dead state) to CL38:portoshim(379) CT1:/ lock=0 ms time=0+0 ms
2024-01-28 12:46:18.392 portod-RO9[1264][26117ebe]: DBG Raw response: error: Success errorMsg: "" get { list { name: "kube-controller-manager-minikube-0889/kube-controller-manager-4349" keyval { variable: "labels" value: "ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIuaGFzaA: NGI5YzUxZmM; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIucmVzdGFydENvdW50: MA; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUG9saWN5: RmlsZQ; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUGF0aA: L2Rldi90ZXJtaW5hdGlvbi1sb2c; ANNOTATION.aW8ua3ViZXJuZXRlcy5wb2QudGVybWluYXRpb25HcmFjZVBlcmlvZA: MzA; LABEL.YXR0ZW1wdA: MA; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubG9ncGF0aA: L3BsYWNlL3BvcnRvL2t1YmUtY29udHJvbGxlci1tYW5hZ2VyLW1pbmlrdWJlLTA4ODkva3ViZS1jb250cm9sbGVyLW1hbmFnZXItNDM0OS9zdGRvdXQ; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubmFtZQ: a3ViZS1jb250cm9sbGVyLW1hbmFnZXI; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZQ: a3ViZS1jb250cm9sbGVyLW1hbmFnZXItbWluaWt1YmU; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZXNwYWNl: a3ViZS1zeXN0ZW0; LABEL.aW8ua3ViZXJuZXRlcy5wb2QudWlk: YzcwNTYzN2Q3MGE0YWE0NGU2MDc4NTlmMDA2YzI2ZjA; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pZA: a3ViZS1jb250cm9sbGVyLW1hbmFnZXItbWluaWt1YmUtMDg4OS9rdWJlLWNvbnRyb2xsZXItbWFuYWdlci00MzQ5; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pbWFnZQ: ZDA1OGFhNWFiOTY5Y2U3Yjg0ZDI1ZTcxODhiZTFmODA2MzNiMThkYjhlYTdkMDJkOWQwYTc4ZTY3NjIzNjU5MQ" } keyval { variable: "state" value: "running" } keyval { variable: "creation_time[raw]" value: "1706445754" } keyval { variable: "start_time[raw]" value: "1706445754" } keyval { variable: "death_time[raw]" error: InvalidState errorMsg: "death_time available only in dead state" } keyval { variable: "exit_code" error: InvalidState errorMsg: "exit_code available only in dead state" } change_time: 1706445754 } } timestamp: 1706445978
2024-01-28 12:46:18.407 portod[1189]:     Connected CL39:portoshim(379) CT1:/ cred=root:root tcred=root:root access=rw ns= wns=
2024-01-28 12:46:18.407 portod-RO0[1255][4aeb15b9]: REQ Version   from CL39:portoshim(379) CT1:/
2024-01-28 12:46:18.407 portod-RO0[1255][4aeb15b9]: DBG Raw request: version { }
2024-01-28 12:46:18.407 portod-RO0[1255][4aeb15b9]: RSP Version   5.3.30-alpha.9 # to CL39:portoshim(379) CT1:/ lock=0 ms time=0+0 ms
2024-01-28 12:46:18.407 portod-RO0[1255][4aeb15b9]: DBG Raw response: error: Success errorMsg: "" version { tag: "5.3.30-alpha.9" revision: "" } timestamp: 1706445978
2024-01-28 12:46:18.407 portod[1189]:     Connected CL43:portoshim(379) CT1:/ cred=root:root tcred=root:root access=rw ns= wns=
2024-01-28 12:46:18.407 portod-RO1[1256][0780f136]: REQ Get   kube-proxy-97nfh-8f2d/kube-proxy-321c -- labels state creation_time[raw] start_time[raw] death_time[raw] exit_code from CL43:portoshim(379) CT1:/
2024-01-28 12:46:18.407 portod-RO1[1256][0780f136]: DBG Raw request: get { name: "kube-proxy-97nfh-8f2d/kube-proxy-321c" variable: "labels" variable: "state" variable: "creation_time[raw]" variable: "start_time[raw]" variable: "death_time[raw]" variable: "exit_code" }
2024-01-28 12:46:18.407 portod-RO1[1256][0780f136]: DBG LockStateRead CT14:kube-proxy-97nfh-8f2d/kube-proxy-321c
2024-01-28 12:46:18.407 portod-RO1[1256][0780f136]: DBG UnlockState CT14:kube-proxy-97nfh-8f2d/kube-proxy-321c
2024-01-28 12:46:18.407 portod-RO1[1256][0780f136]: RSP Get   kube-proxy-97nfh-8f2d/kube-proxy-321c -- labels state creation_time[raw] start_time[raw] death_time[raw] exit_code kube-proxy-97nfh-8f2d/kube-proxy-321c: labels=ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIuaGFzaA: ODliYzllMmY; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIucmVzdGFydENvdW50: MA; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUG9saWN5: RmlsZQ; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUGF0aA: L2Rldi90ZXJtaW5hdGlvbi1sb2c; ANNOTATION.aW8ua3ViZXJuZXRlcy5wb2QudGVybWluYXRpb25HcmFjZVBlcmlvZA: MzA; LABEL.YXR0ZW1wdA: MA; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubG9ncGF0aA: L3BsYWNlL3BvcnRvL2t1YmUtcHJveHktOTduZmgtOGYyZC9rdWJlLXByb3h5LTMyMWMvc3Rkb3V0; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubmFtZQ: a3ViZS1wcm94eQ; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZQ: a3ViZS1wcm94eS05N25maA; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZXNwYWNl: a3ViZS1zeXN0ZW0; LABEL.aW8ua3ViZXJuZXRlcy5wb2QudWlk: YWEwZDc4YTgtZmRkZS00ZTVjLWFhMTEtMzhmYmUzY2RlOGYy; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pZA: a3ViZS1wcm94eS05N25maC04ZjJkL2t1YmUtcHJveHktMzIxYw; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pbWFnZQ: ODNmNmNjNDA3ZWVkODhkMjE0YWFkOTdmMzUzOWJkZTVjOGU0ODVmZjE0NDI0Y2QwMjFhM2EyODk5MzA0Mzk4ZQ state=running creation_time[raw]=1706445772 start_time[raw]=1706445772 death_time[raw]=8:InvalidState(death_time available only in dead state) exit_code=8:InvalidState(exit_code available only in dead state) to CL43:portoshim(379) CT1:/ lock=0 ms time=0+0 ms
2024-01-28 12:46:18.407 portod-RO1[1256][0780f136]: DBG Raw response: error: Success errorMsg: "" get { list { name: "kube-proxy-97nfh-8f2d/kube-proxy-321c" keyval { variable: "labels" value: "ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIuaGFzaA: ODliYzllMmY; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIucmVzdGFydENvdW50: MA; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUG9saWN5: RmlsZQ; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUGF0aA: L2Rldi90ZXJtaW5hdGlvbi1sb2c; ANNOTATION.aW8ua3ViZXJuZXRlcy5wb2QudGVybWluYXRpb25HcmFjZVBlcmlvZA: MzA; LABEL.YXR0ZW1wdA: MA; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubG9ncGF0aA: L3BsYWNlL3BvcnRvL2t1YmUtcHJveHktOTduZmgtOGYyZC9rdWJlLXByb3h5LTMyMWMvc3Rkb3V0; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubmFtZQ: a3ViZS1wcm94eQ; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZQ: a3ViZS1wcm94eS05N25maA; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZXNwYWNl: a3ViZS1zeXN0ZW0; LABEL.aW8ua3ViZXJuZXRlcy5wb2QudWlk: YWEwZDc4YTgtZmRkZS00ZTVjLWFhMTEtMzhmYmUzY2RlOGYy; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pZA: a3ViZS1wcm94eS05N25maC04ZjJkL2t1YmUtcHJveHktMzIxYw; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pbWFnZQ: ODNmNmNjNDA3ZWVkODhkMjE0YWFkOTdmMzUzOWJkZTVjOGU0ODVmZjE0NDI0Y2QwMjFhM2EyODk5MzA0Mzk4ZQ" } keyval { variable: "state" value: "running" } keyval { variable: "creation_time[raw]" value: "1706445772" } keyval { variable: "start_time[raw]" value: "1706445772" } keyval { variable: "death_time[raw]" error: InvalidState errorMsg: "death_time available only in dead state" } keyval { variable: "exit_code" error: InvalidState errorMsg: "exit_code available only in dead state" } change_time: 1706445772 } } timestamp: 1706445978
2024-01-28 12:46:18.423 portod[1189]:     Connected CL45:portoshim(379) CT1:/ cred=root:root tcred=root:root access=rw ns= wns=
2024-01-28 12:46:18.423 portod-RO4[1259][2f1e9302]: REQ Version   from CL45:portoshim(379) CT1:/
2024-01-28 12:46:18.423 portod-RO4[1259][2f1e9302]: DBG Raw request: version { }
2024-01-28 12:46:18.423 portod-RO4[1259][2f1e9302]: RSP Version   5.3.30-alpha.9 # to CL45:portoshim(379) CT1:/ lock=0 ms time=0+0 ms
2024-01-28 12:46:18.423 portod-RO4[1259][2f1e9302]: DBG Raw response: error: Success errorMsg: "" version { tag: "5.3.30-alpha.9" revision: "" } timestamp: 1706445978
2024-01-28 12:46:18.424 portod[1189]:     Connected CL47:portoshim(379) CT1:/ cred=root:root tcred=root:root access=rw ns= wns=
2024-01-28 12:46:18.424 portod-RO8[1263][7620f8b0]: REQ Get   kube-scheduler-minikube-463a/kube-scheduler-9b75 -- labels state creation_time[raw] start_time[raw] death_time[raw] exit_code from CL47:portoshim(379) CT1:/
2024-01-28 12:46:18.424 portod-RO8[1263][7620f8b0]: DBG Raw request: get { name: "kube-scheduler-minikube-463a/kube-scheduler-9b75" variable: "labels" variable: "state" variable: "creation_time[raw]" variable: "start_time[raw]" variable: "death_time[raw]" variable: "exit_code" }
2024-01-28 12:46:18.424 portod-RO8[1263][7620f8b0]: DBG LockStateRead CT9:kube-scheduler-minikube-463a/kube-scheduler-9b75
2024-01-28 12:46:18.424 portod-RO8[1263][7620f8b0]: DBG UnlockState CT9:kube-scheduler-minikube-463a/kube-scheduler-9b75
2024-01-28 12:46:18.424 portod-RO8[1263][7620f8b0]: RSP Get   kube-scheduler-minikube-463a/kube-scheduler-9b75 -- labels state creation_time[raw] start_time[raw] death_time[raw] exit_code kube-scheduler-minikube-463a/kube-scheduler-9b75: labels=ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIuaGFzaA: ZTE2MzljN2E; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIucmVzdGFydENvdW50: MA; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUG9saWN5: RmlsZQ; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUGF0aA: L2Rldi90ZXJtaW5hdGlvbi1sb2c; ANNOTATION.aW8ua3ViZXJuZXRlcy5wb2QudGVybWluYXRpb25HcmFjZVBlcmlvZA: MzA; LABEL.YXR0ZW1wdA: MA; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubG9ncGF0aA: L3BsYWNlL3BvcnRvL2t1YmUtc2NoZWR1bGVyLW1pbmlrdWJlLTQ2M2Eva3ViZS1zY2hlZHVsZXItOWI3NS9zdGRvdXQ; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubmFtZQ: a3ViZS1zY2hlZHVsZXI; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZQ: a3ViZS1zY2hlZHVsZXItbWluaWt1YmU; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZXNwYWNl: a3ViZS1zeXN0ZW0; LABEL.aW8ua3ViZXJuZXRlcy5wb2QudWlk: MDdkN2QyOWEwYmJjMGNiMWFmYmY5YWE3ZDcxOTkzNjM; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pZA: a3ViZS1zY2hlZHVsZXItbWluaWt1YmUtNDYzYS9rdWJlLXNjaGVkdWxlci05Yjc1; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pbWFnZQ: ZTNkYjMxM2M2ZGJjMDY1ZDRhYzNiMzJjN2E2ZjJhODc4OTQ5MDMxYjg4MWQyMTdiNjM4ODFhMTA5YzVjZmJhMQ state=running creation_time[raw]=1706445754 start_time[raw]=1706445754 death_time[raw]=8:InvalidState(death_time available only in dead state) exit_code=8:InvalidState(exit_code available only in dead state) to CL47:portoshim(379) CT1:/ lock=0 ms time=0+0 ms
2024-01-28 12:46:18.424 portod-RO8[1263][7620f8b0]: DBG Raw response: error: Success errorMsg: "" get { list { name: "kube-scheduler-minikube-463a/kube-scheduler-9b75" keyval { variable: "labels" value: "ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIuaGFzaA: ZTE2MzljN2E; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIucmVzdGFydENvdW50: MA; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUG9saWN5: RmlsZQ; ANNOTATION.aW8ua3ViZXJuZXRlcy5jb250YWluZXIudGVybWluYXRpb25NZXNzYWdlUGF0aA: L2Rldi90ZXJtaW5hdGlvbi1sb2c; ANNOTATION.aW8ua3ViZXJuZXRlcy5wb2QudGVybWluYXRpb25HcmFjZVBlcmlvZA: MzA; LABEL.YXR0ZW1wdA: MA; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubG9ncGF0aA: L3BsYWNlL3BvcnRvL2t1YmUtc2NoZWR1bGVyLW1pbmlrdWJlLTQ2M2Eva3ViZS1zY2hlZHVsZXItOWI3NS9zdGRvdXQ; LABEL.aW8ua3ViZXJuZXRlcy5jb250YWluZXIubmFtZQ: a3ViZS1zY2hlZHVsZXI; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZQ: a3ViZS1zY2hlZHVsZXItbWluaWt1YmU; LABEL.aW8ua3ViZXJuZXRlcy5wb2QubmFtZXNwYWNl: a3ViZS1zeXN0ZW0; LABEL.aW8ua3ViZXJuZXRlcy5wb2QudWlk: MDdkN2QyOWEwYmJjMGNiMWFmYmY5YWE3ZDcxOTkzNjM; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pZA: a3ViZS1zY2hlZHVsZXItbWluaWt1YmUtNDYzYS9rdWJlLXNjaGVkdWxlci05Yjc1; LABEL.cG9ydG9zaGltLmNvbnRhaW5lci5pbWFnZQ: ZTNkYjMxM2M2ZGJjMDY1ZDRhYzNiMzJjN2E2ZjJhODc4OTQ5MDMxYjg4MWQyMTdiNjM4ODFhMTA5YzVjZmJhMQ" } keyval { variable: "state" value: "running" } keyval { variable: "creation_time[raw]" value: "1706445754" } keyval { variable: "start_time[raw]" value: "1706445754" } keyval { variable: "death_time[raw]" error: InvalidState errorMsg: "death_time available only in dead state" } keyval { variable: "exit_code" error: InvalidState errorMsg: "exit_code available only in dead state" } change_time: 1706445754 } } timestamp: 1706445978

